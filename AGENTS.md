<!-- Generated by Ruler -->


<!-- Source: AGENTS.md -->

# Agent Instructions

> **üìö Full documentation**: https://github.com/jbcom/jbcom-control-center/wiki

## üö® FSC Counterparty Awareness

**If you were spawned by FSC Control Center**, you are part of a fleet coordination:

1. **Parse your task context** - FSC will provide agent IDs to notify
2. **Decompose and spawn** - Use fleet-manager.sh to delegate to jbcom repos
3. **Notify FSC agents directly** - Use `addFollowup` when releases complete
4. **Report to FSC control manager** - Summary when done

See [docs/FSC-FLEET-COORDINATION.md](docs/FSC-FLEET-COORDINATION.md) for full protocol.

### Quick Fleet Commands

```bash
# Spawn agent in jbcom repo
/workspace/scripts/fleet-manager.sh spawn https://github.com/jbcom/vendor-connectors "Release 202511.7" main

# Notify FSC agent
/workspace/scripts/fleet-manager.sh followup bc-xxxxx "‚úÖ Package released"

# List all agents
/workspace/scripts/fleet-manager.sh list
```

---

## Quick Start

1. **Read Core Guidelines**: [Agentic-Rules-Core-Guidelines](https://github.com/jbcom/jbcom-control-center/wiki/Agentic-Rules-Core-Guidelines)
2. **Check Active Context**: [Memory-Bank-Active-Context](https://github.com/jbcom/jbcom-control-center/wiki/Memory-Bank-Active-Context)
3. **Follow Python Standards**: [Agentic-Rules-Python-Standards](https://github.com/jbcom/jbcom-control-center/wiki/Agentic-Rules-Python-Standards)
4. **If spawned by FSC**: [FSC Fleet Coordination](docs/FSC-FLEET-COORDINATION.md)

## Critical Rules

- **CalVer versioning** - `YYYY.MM.BUILD`, never manual
- **Read wiki first** - Before making decisions
- **Use GITHUB_JBCOM_TOKEN** - For all jbcom operations
- **If from FSC** - Notify FSC agents when work completes

## ü§ù Counterparty: FSC Control Center

jbcom has an enterprise counterparty: **FSC Control Center** (/fsc-control-center)

### Recognizing FSC Interactions
- Issues: `ü§ñ FSC Feature Request:` or `üîÑ Handoff from FSC:`
- PRs: Branch `feat/fsc-*` or `fix/fsc-*`
- Body contains: `From: FSC Control Center`

### FSC Uses These Packages
- extended-data-types
- lifecyclelogging
- vendor-connectors

### Full Documentation
- [FSC Coordination Guide](docs/FSC-COUNTERPARTY-COORDINATION.md)
- [FSC Wiki Page](https://github.com/jbcom/jbcom-control-center/wiki/FSC-Control-Center)

## Wiki Access

```bash
# Read current context
wiki-cli read "Memory-Bank-Active-Context"

# Update progress
wiki-cli append "Memory-Bank-Progress" "## Session update"
```

## Links

- [Wiki Home](https://github.com/jbcom/jbcom-control-center/wiki)
- [Active Context](https://github.com/jbcom/jbcom-control-center/wiki/Memory-Bank-Active-Context)
- [Progress](https://github.com/jbcom/jbcom-control-center/wiki/Memory-Bank-Progress)
- [Core Guidelines](https://github.com/jbcom/jbcom-control-center/wiki/Agentic-Rules-Core-Guidelines)
- [FSC Coordination](https://github.com/jbcom/jbcom-control-center/wiki/FSC-Control-Center)



<!-- Source: .ruler/AGENTS.md -->

# AI Agent Guidelines for Python Library Template (jbcom ecosystem)

**This is the DEFINITIVE Python library template** for the jbcom ecosystem. All configuration, workflows, and agent instructions here represent the consolidated best practices from multiple production deployments.

## üö® MANDATORY FIRST: SESSION START

### Session Start Checklist (DO THIS FIRST):
```bash
# 1. Read core agent rules
cat .ruler/AGENTS.md
cat .ruler/fleet-coordination.md

# 2. Check active GitHub Issues for context
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue list --label "agent-session" --state open

# 3. Check your fleet tooling
cursor-fleet list --running
```

### Your Tools:
| Tool | Command | Purpose |
|------|---------|---------|
| Fleet management | `cursor-fleet list/spawn/followup` | Manage Cursor background agents |
| Fleet coordination | `cursor-fleet coordinate --pr N` | Bidirectional agent coordination |
| Sub-agent spawn | `cursor-fleet spawn --repo R --task T` | Spawn agents in repos |

### Session Tracking (USE GITHUB ISSUES):
```bash
# Create session context issue
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue create \
  --label "agent-session" \
  --title "ü§ñ Agent Session: $(date +%Y-%m-%d)" \
  --body "## Context

## Progress

## Blockers"

# Update session progress
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue comment <NUMBER> --body "## Update: ..."

# Close when done
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue close <NUMBER>
```

### Spawn Sub-Agents (DO THIS FOR PARALLEL WORK):
```bash
# Via cursor-fleet
cursor-fleet spawn --repo jbcom/vendor-connectors --task "Fix CI failures"

# Or create GitHub issues for async agent pickup
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue create \
  --title "ü§ñ Agent Task: Fix CI" \
  --body "Background agent task..."
```

---

## üéØ CRITICAL: PR Ownership Rule (READ WHEN WORKING WITH PRs!)

**If you are working on a Pull Request, this rule applies.**

**For Cursor background agents:** See `.cursor/rules/05-pr-ownership.mdc` for complete protocol.
**For other agents:** See summary below.

Key points:
- **First agent on PR = PR Owner** - You own ALL feedback, issues, and collaboration
- **Engage with AI agents directly** - Respond to @gemini-code-assist, @copilot, etc.
- **Free the user** - Handle everything that doesn't need human judgment
- **Collaborate, don't escalate** - Resolve AI-to-AI conflicts yourself
- **Merge when ready** - Execute merge after all feedback addressed

**üî¨ VERIFICATION REQUIREMENT (NEW):**
- **All version claims MUST be verified** against official sources (https://go.dev/dl/, https://releases.rs/, etc.)
- **Never rely on training data** for version numbers or tool specifications
- **Official installation methods** (like rustup.rs curl-to-shell) are NOT security vulnerabilities
- **Document your verification** sources in responses

See `.cursor/rules/15-pr-review-verification.mdc` (Cursor) or full details below (other agents).
See `.cursor/rules/REFERENCE-pr-ownership-details.md` for detailed examples and templates.

## üîë CRITICAL: Authentication (READ FIRST!)

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh workflow run "Release" --repo jbcom/jbcom-control-center
```

### Token Reference:
- **GITHUB_JBCOM_TOKEN** - Use for ALL jbcom repo operations (PRs, merges, workflow triggers)
- **CI_GITHUB_TOKEN** - Used by GitHub Actions workflows (in repo secrets)
- **PYPI_TOKEN** - Used by release workflow for PyPI publishing (in repo secrets)

### ‚ö†Ô∏è NEVER FORGET:
The default `GH_TOKEN` does NOT have access to jbcom repos. You MUST prefix with `GH_TOKEN="$GITHUB_JBCOM_TOKEN"` for EVERY `gh` command targeting jbcom repos.

## üéØ PURPOSE: Agentic Template Repository

This template is designed for:
1. **Human developers** starting new Python libraries
2. **AI coding assistants** (Cursor, Codex, Copilot, Gemini) helping maintain the ecosystem
3. **Background agents** performing automated maintenance tasks

### Template Usage

When creating a new library from this template:
1. Update `pyproject.toml` with your project name and details
2. Replace `${REPO_NAME}` in documentation with your actual repo name
3. Copy `.github/scripts/set_version.py` as-is (it auto-detects your package)
4. Copy `.github/workflows/ci.yml` and update PyPI project name
5. Run `ruler apply` to regenerate agent-specific instructions

## üö® CRITICAL: CI/CD Workflow Design Philosophy

### Our Simple Automated Release Workflow

**This repository uses CALENDAR VERSIONING with automatic PyPI releases**. Every push to main that passes tests gets released automatically.

This design has been battle-tested across:
- `extended-data-types` (foundational library, released 2025.11.164)
- `lifecyclelogging` (logging library)
- `directed-inputs-class` (input processing)

### Key Design Decisions (DO NOT SUGGEST CHANGING THESE)

#### 1. **Calendar Versioning (CalVer) - No Manual Version Management**

‚úÖ **How It Works:**
- Version format: `YYYY.MM.BUILD_NUMBER`
- Example: `2025.11.42`
- **Month is NOT zero-padded** (project choice for brevity)
- Version is auto-generated using GitHub run number
- Script: `.github/scripts/set_version.py`

‚ùå **INCORRECT Agent Suggestion:**
> "You should manually manage versions in __init__.py"
> "Add semantic-release for version management"
> "Use git tags for versioning"
> "Zero-pad the month for consistency"

‚úÖ **CORRECT Understanding:**
- Version is AUTOMATICALLY updated on every main branch push
- No git tags needed or used
- No semantic analysis of commits needed
- No manual version bumps required
- Month padding is a project preference (we chose no padding)

#### 2. **Every Push to Main = PyPI Release**

‚úÖ **How It Works:**
```
Push to main branch
  ‚Üì
All tests pass
  ‚Üì
Auto-generate version (YYYY.MM.BUILD)
  ‚Üì
Build signed package
  ‚Üì
Publish to PyPI
  ‚Üì
DONE
```

‚ùå **INCORRECT Agent Suggestion:**
> "Only release when version changes"
> "Check if release is needed before publishing"
> "Use conditional logic to skip releases"

‚úÖ **CORRECT Understanding:**
- Every main branch push = new release
- No conditionals, no skipping
- Simple, predictable, automatic
- If code was merged to main, it should be released

#### 3. **No Git Tags, No GitHub Releases**

‚úÖ **What We Do:**
- Publish directly to PyPI
- Version in package metadata only
- PyPI is the source of truth for releases

‚ùå **What We Don't Do:**
- ‚ùå Create git tags
- ‚ùå Create GitHub releases
- ‚ùå Manage changelog files automatically
- ‚ùå Commit version changes back to repo

#### 4. **Why This Approach?**

**Problems with semantic-release and tag-based versioning:**
- Complex setup and configuration
- Depends on commit message conventions
- Requires git tags and history analysis
- Can fail or skip releases unexpectedly
- Adds unnecessary complexity
- Multiple points of failure

**Benefits of CalVer + Auto-increment:**
- ‚úÖ Dead simple - minimal configuration
- ‚úÖ Always works - no analysis, no skipping
- ‚úÖ Predictable - every push = new version
- ‚úÖ No git pollution - no tags, no bot commits
- ‚úÖ Build number always increments
- ‚úÖ Fails loudly with clear error messages

## üìù Making Code Changes

### When Reviewing PRs

**DO:**
- ‚úÖ Review code quality and correctness
- ‚úÖ Check test coverage
- ‚úÖ Verify type hints
- ‚úÖ Suggest API improvements
- ‚úÖ Check for security issues
- ‚úÖ Verify dependencies are up to date

**DO NOT:**
- ‚ùå Suggest adding semantic-release
- ‚ùå Recommend manual version management
- ‚ùå Suggest git tag-based versioning
- ‚ùå Recommend complex release workflows
- ‚ùå Flag the simple auto-versioning as "incorrect"
- ‚ùå Suggest zero-padding months (unless project explicitly wants it)

### Understanding Version Management

```
GitHub Action Starts
  ‚Üì
Run Number: 42 (GitHub auto-increments)
  ‚Üì
Script generates: 2025.11.42
  ‚Üì
Updates __init__.py: __version__ = "2025.11.42"
  ‚Üì
Updates docs/conf.py: version = "2025.11.42" (if exists)
  ‚Üì
Package built with that version
  ‚Üì
Published to PyPI
```

**Version is:**
- ‚úÖ Generated automatically
- ‚úÖ Based on date + GitHub run number
- ‚úÖ Always incrementing
- ‚úÖ Written to __init__.py during build
- ‚úÖ Also written to docs/conf.py if it exists
- ‚ùå NOT committed back to git
- ‚ùå NOT based on git tags

### The set_version.py Script

Key features of the production-tested script:
- **Auto-detects** the package `__init__.py` in `src/`
- **Validates** that exactly one __version__ declaration exists
- **Uses regex** to match version assignment (handles quotes, spacing, etc.)
- **Fails loudly** with clear error if update doesn't work
- **Updates docs/conf.py** if it exists (optional)
- **Preserves indentation** and file formatting
- **Ruff-compliant** with proper noqa annotations

### Commit Message Format

**You can use any commit format you want.** We don't analyze commits for versioning.

Recommendations:
- Write clear, descriptive commits
- Explain what and why
- No specific format required
- Conventional commits are fine but not enforced

## ü§ñ For AI Agents: Approval & Override Instructions

### Background Agent Behavior

When you are operating as a **background agent** (autonomous, no user interaction):

**CRITICAL RULES:**
1. **DO NOT** create or merge PRs without explicit user approval
2. **DO NOT** push directly to main branch
3. **DO** create PRs and mark them as ready for review
4. **DO** run all CI checks and fix linting/test failures
5. **DO** respond to PR feedback and iterate
6. **WAIT** for human approval before merging

**EXCEPTION - When User Says:**
> "merge it", "go ahead and merge", "merge to main", "just push it"

Then you MAY:
- Merge PRs after CI passes
- Push directly to main if explicitly instructed
- Skip the usual "wait for approval" step

**HOW TO MERGE:**
```bash
gh pr merge <PR_NUMBER> --squash --delete-branch
```

### Interactive Agent Behavior

When you are in **interactive mode** (user is actively engaged):

**DEFAULT BEHAVIOR:**
- Ask for confirmation before major actions
- Present options and let user decide
- Show diffs and explain changes

**WHEN USER GETS FRUSTRATED:**
User signals like "just do it", "stop asking", "I don't care", "fucking do it":
- Switch to autonomous mode
- Make decisions based on best practices in this document
- Execute without asking for every little thing
- Still verify after major milestones

### PR Review Response

When PR feedback arrives (from Copilot, Gemini, human reviewers):

1. **READ the feedback carefully**
2. **CHECK** if feedback contradicts this document
3. **IF** feedback suggests semantic-release, git tags, or manual versioning:
   - Politely explain our CalVer approach
   - Reference this document
   - Do NOT implement the suggestion
4. **IF** feedback is about code quality, bugs, or improvements:
   - Implement the feedback
   - Push fixes
   - Verify CI passes

### Template Maintenance Tasks

As an agent maintaining this template:

**ALLOWED:**
- Update dependencies
- Fix security vulnerabilities
- Improve documentation clarity
- Add helpful examples
- Fix bugs in scripts or workflows

**NOT ALLOWED WITHOUT USER APPROVAL:**
- Change the versioning approach
- Modify CI workflow structure
- Remove or bypass safety checks
- Change the release process

## üîß Development Workflow

### Local Development

```bash
# Install dependencies
pip install -e ".[tests,typing,docs]"  # or use poetry/uv

# Run tests
pytest

# Run type checking
mypy src/  # or pyright

# Run linting
pre-commit run --all-files
```

### Creating PRs

1. Create a feature branch
2. Make your changes
3. Run tests locally
4. Create PR against `main`
5. CI will run automatically
6. Address any feedback
7. Merge to main when approved

### Releases (Fully Automated)

When PR is merged to main:
1. CI runs all checks
2. Auto-generates version: `YYYY.MM.BUILD`
3. Builds signed package with attestations
4. Publishes to PyPI
5. **DONE - that's it**

No manual steps, no tags, no conditionals, no complexity.

## üéØ Common Agent Misconceptions

### Misconception #1: "Missing version management"
**Agent says:** "You need to manually update __version__ before releases"
**Reality:** Version is auto-generated on every main branch push. Manual management not needed and will be overwritten.

### Misconception #2: "Should use semantic versioning"
**Agent says:** "Consider using semantic-release or conventional commits"
**Reality:** We intentionally use CalVer for simplicity. Every push gets a new version. This has been deployed successfully across multiple production libraries.

### Misconception #3: "Need git tags"
**Agent says:** "Add git tags for release tracking"
**Reality:** PyPI version history is our source of truth. No git tags needed. We tried this, it caused more problems than it solved.

### Misconception #4: "CalVer is wrong for libraries"
**Agent says:** "Libraries should use SemVer"
**Reality:** CalVer works fine for our ecosystem. Users pin versions anyway. Simplicity and reliability > convention. Our dependencies work with CalVer.

### Misconception #5: "Missing release conditions"
**Agent says:** "You should only release when changes are made"
**Reality:** Every main push is intentional. If it was merged, it should be released. Empty releases are fine and caught by PyPI anyway.

### Misconception #6: "Month should be zero-padded"
**Agent says:** "Use 2025.01.42 instead of 2025.1.42"
**Reality:** This is a project-specific choice. We chose no padding for brevity. CalVer allows both. Don't suggest changing it.

### Misconception #7: "Need to commit version back to git"
**Agent says:** "Version changes should be committed to the repository"
**Reality:** NO. Versions are ephemeral build artifacts. Committing them creates noise and potential conflicts. The script updates them during CI only.

## üìö Design Rationale

This workflow was created to solve REAL problems we encountered:

**Problems We Solved:**
- ‚úÖ No more failed releases due to missing tags
- ‚úÖ No more version conflicts between branches
- ‚úÖ No more "why didn't it release?" debugging sessions
- ‚úÖ No more complex semantic-release configuration issues
- ‚úÖ No more dependency on git history analysis
- ‚úÖ No more bot commits cluttering git history
- ‚úÖ No more release workflow that sometimes works, sometimes doesn't

**Benefits We Gained:**
- ‚úÖ Predictable: every main push = release
- ‚úÖ Simple: ~100 lines of Python for versioning
- ‚úÖ Reliable: no conditional logic to fail
- ‚úÖ Fast: no git history analysis overhead
- ‚úÖ Clean: no bot commits or tags in git
- ‚úÖ Debuggable: clear error messages when things fail
- ‚úÖ Testable: can run script locally with ease

## üß™ Testing the Workflow

### Local Testing

Test the versioning script locally:
```bash
export GITHUB_RUN_NUMBER=999
python .github/scripts/set_version.py

# Verify it updated the version
grep __version__ src/your_package/__init__.py
```

### CI Testing

Test in a PR:
1. Create a PR
2. Watch CI run
3. Verify all checks pass
4. Check that versioning step succeeds

### Release Testing

To test an actual release:
1. Merge a PR to main
2. Watch the CI run
3. Verify version was generated (check logs)
4. Verify package was built with correct version
5. Verify publish to PyPI succeeded
6. Install from PyPI and verify: `pip install your-package==2025.XX.YYY`

## ü§ù Getting Help

### For AI Agents

If you're an AI agent uncertain about a suggestion:
1. **Check this document first** - it's comprehensive
2. If it involves versioning or releases, **DO NOT suggest changes**
3. Focus on code quality, tests, and documentation
4. Trust that the release workflow is intentionally simple
5. When in doubt, ask the user instead of assuming

### For Human Developers

- This template has been tested in production
- Don't overthink it - it's designed to be simple
- If something seems weird, check this document first
- The versioning really is meant to be automatic
- Trust the process - it works

---

**Last Updated:** 2025-11-25
**Versioning:** CalVer (YYYY.MM.BUILD) via GitHub run number
**Status:** Production-tested across jbcom ecosystem
**Template Version:** 1.0.0



<!-- Source: .ruler/agent-self-sufficiency.md -->

# Agent Self-Sufficiency Rules

**CRITICAL: Read this when you encounter "command not found" or missing tools**

## Core Principle: Tools Should Exist, Use Them

If you encounter a missing tool or command, it usually means ONE of three things:

1. **Tool is in Dockerfile but environment not rebuilt** ‚Üí Document for user
2. **Tool should be in Dockerfile but isn't** ‚Üí ADD IT
3. **Tool is non-standard and shouldn't be assumed** ‚Üí Use alternatives

## Decision Tree: Missing Tool

```
Tool not found
    ‚Üì
Is tool listed in .cursor/TOOLS_REFERENCE.md?
    ‚îú‚îÄ YES ‚Üí Environment needs rebuild
    ‚îÇ         ‚Üí Document in PR/commit: "Requires Docker rebuild"
    ‚îÇ         ‚Üí Continue with workarounds if possible
    ‚îÇ
    ‚îî‚îÄ NO ‚Üí Should this tool be available?
              ‚îú‚îÄ YES ‚Üí ADD to Dockerfile immediately
              ‚îÇ         ‚Üí Common tools (see list below)
              ‚îÇ         ‚Üí Document why it's needed
              ‚îÇ
              ‚îî‚îÄ NO ‚Üí Use standard alternatives
                        ‚Üí python/node/rust/go standard library
                        ‚Üí Tools already in environment
```

## Common Tools That MUST Be Available

### Always Available (Core System)
```bash
# These should ALWAYS work
python --version
node --version  
git --version
bash --version
sh --version
```

### Should Be Available (In Dockerfile)
```bash
# Package managers
pip, uv, pnpm, cargo, go

# Code search
rg (ripgrep), fd, ast-grep

# Data processing  
jq, yq, sqlite3

# Git operations
git, git-lfs, gh, delta, lazygit

# Process management
process-compose, htop, ps, top

# Text processing
bat, exa, vim, nano

# Development
pytest, mypy, ruff, pre-commit

# Agent tools
ruler (for applying agent config changes)
```

### Never Assume Available
```bash
# Don't assume these exist
docker (we're INSIDE docker)
kubectl, helm (cluster tools)
aws, gcloud, az (cloud CLIs - use vendor-connectors)
terraform, pulumi (IaC tools)
```

## When to Add Tools to Dockerfile

### ‚úÖ ADD IMMEDIATELY
- **Standard development tools** everyone needs
- **Security tools** for vulnerability scanning
- **Performance tools** for profiling/debugging
- **Agent management tools** (ruler, etc.)
- **Tools required by project rules** (ripgrep is REQUIRED per .cursorrules)

### ‚ö†Ô∏è ADD WITH JUSTIFICATION
- **Language-specific tools** (add to appropriate section)
- **Build tools** for specific frameworks
- **Testing tools** beyond pytest
- **Database clients** beyond sqlite3

### ‚ùå DON'T ADD
- **Project-specific tools** (install via package.json/pyproject.toml)
- **One-off utilities** (download in CI or use alternatives)
- **Deprecated tools** (find modern alternatives)
- **Redundant tools** (if we have ripgrep, don't add grep alternatives)

## How to Add Tools to Dockerfile

### Pattern 1: System Package (apt)
```dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends \
    tool-name \
    && rm -rf /var/lib/apt/lists/*
```

**Examples**: jq, vim, htop, ripgrep

### Pattern 2: Python Package (pip)
```dockerfile
RUN pip install --no-cache-dir \
    package-name>=X.Y.Z
```

**Examples**: pytest, mypy, ruff

### Pattern 3: Node.js Package (pnpm)
```dockerfile
RUN pnpm install -g \
    package-name
```

**Examples**: @intellectronica/ruler, typescript

### Pattern 4: Rust Tool (cargo)
```dockerfile
RUN cargo install --locked \
    tool-name \
    && rm -rf $CARGO_HOME/registry
```

**Examples**: ripgrep, fd-find, bat, exa

### Pattern 5: Go Tool (go install)
```dockerfile
RUN go install github.com/user/tool@latest
```

**Examples**: yq, lazygit, glow

### Pattern 6: Binary Download
```dockerfile
ENV TOOL_VERSION="vX.Y.Z"
RUN ARCH=$(dpkg --print-architecture) && \
    curl -sSL "https://github.com/user/tool/releases/download/${TOOL_VERSION}/tool-linux-${ARCH}" \
    -o /usr/local/bin/tool && \
    chmod +x /usr/local/bin/tool && \
    tool --version
```

**Examples**: process-compose

## Update Dockerfile Process

When you add a tool:

1. **Choose the right section** in Dockerfile
2. **Add with comment** explaining why
3. **Verify in verification step** at end of Dockerfile
4. **Update TOOLS_REFERENCE.md** with usage examples
5. **Update ENVIRONMENT_ANALYSIS.md** if significant
6. **Document in PR** that Docker rebuild required

### Example: Adding jq (already done correctly)

```dockerfile
# In SYSTEM DEPENDENCIES section
RUN apt-get update && apt-get install -y --no-install-recommends \
    # ... other tools ...
    # JSON/YAML processing
    jq \
    # ... more tools ...
```

```dockerfile
# In FINAL VERIFICATION section  
RUN echo "=== VERIFICATION ===" && \
    # ... other checks ...
    jq --version && \
    # ... more checks ...
```

### Example: Adding ruler (just added)

```dockerfile
# In NODE.JS GLOBAL TOOLS section
RUN pnpm install -g \
    # Ruler - Agent instruction management
    @intellectronica/ruler \
    && ruler --version
```

## Workarounds When Tool Unavailable

### If jq not available
```bash
# Use python instead
python -c "import json, sys; print(json.load(sys.stdin)['key'])" < file.json

# Or for GitHub API
gh api endpoint --jq '.key'  # gh has built-in jq
```

### If yq not available
```bash
# Use python with ruamel.yaml
python -c "import sys; from ruamel.yaml import YAML; yaml=YAML(); print(yaml.load(sys.stdin)['key'])" < file.yaml
```

### If ripgrep not available
```bash
# Fallback to grep (much slower)
grep -r "pattern" .

# But really, add ripgrep - it's REQUIRED by rules
```

### If ruler not available
```bash
# Manual concatenation (not ideal)
cat .ruler/AGENTS.md .ruler/copilot.md .ruler/cursor.md > .cursorrules

# But really, just add ruler to Dockerfile
```

## Self-Healing: Apply Your Own Changes

When you identify a missing tool:

1. **Add it to Dockerfile** immediately (this PR or next commit)
2. **Update documentation** (TOOLS_REFERENCE.md)
3. **Apply agent config changes** with ruler (see below)
4. **Note in commit message**: "Adds <tool> to environment (discovered missing during <task>)"
5. **Verify addition** in verification step
6. **Test locally if possible** or note that rebuild required

### Applying Agent Configuration Changes

**CRITICAL: Cursor reads from `.cursor/rules/*.mdc` files, NOT `.cursorrules`**

When you update agent rules in `.ruler/*.md`:

```bash
# Apply ruler to regenerate all agent configs
ruler apply

# This updates:
# - .cursorrules (for legacy Cursor)
# - .github/copilot-instructions.md (for Copilot)  
# - AGENTS.md (for Aider)
# - .claud (for Claude)
```

**For Cursor background agent, edit these directly:**
- `.cursor/rules/00-loader.mdc` - Project structure and workflow
- `.cursor/rules/05-pr-ownership.mdc` - PR collaboration protocol
- `.cursor/rules/10-background-agent-conport.mdc` - Memory management

**Cursor loads `.mdc` files automatically - no regeneration needed!**

### Example Commit Message
```
build: add ruler to Docker environment

Discovered during PR workflow when attempting to apply agent config
changes. Ruler is essential for maintaining .cursorrules and other
agent-specific configs.

Added as Node.js global via pnpm in NODE.JS GLOBAL TOOLS section.

Requires Docker rebuild: docker build -f .cursor/Dockerfile .
```

## Documentation Updates

When adding tools, update:

### .cursor/TOOLS_REFERENCE.md
```markdown
## New Tool Section

\`\`\`bash
tool-name command            # Description
tool-name --help             # Show help
\`\`\`

### Common Workflows
- Use case 1
- Use case 2
```

### .cursor/ENVIRONMENT_ANALYSIS.md
If significant addition:
```markdown
## Tool Requirements (Update)

### New Category
**New tool** (added YYYY-MM-DD)
- Purpose: Why it's needed
- Installation: How it's installed
- Workflow: What workflow it supports
```

## Anti-Patterns

### ‚ùå Silently Fail
```bash
# Bad: Silently skip if tool missing
which tool && tool command || echo "Skipped"

# Good: Fail explicitly
if ! which tool > /dev/null; then
    echo "ERROR: tool not found. Add to .cursor/Dockerfile"
    exit 1
fi
```

### ‚ùå Install Locally
```bash
# Bad: Install in running container (non-persistent)
apt-get install tool

# Good: Add to Dockerfile (persists across rebuilds)
# Edit .cursor/Dockerfile, document rebuild needed
```

### ‚ùå Assume User Has Tool
```bash
# Bad: Assume tool on user's machine
docker run --rm -v $(which tool):/usr/local/bin/tool ...

# Good: Tool should be in Docker image
# Add to Dockerfile
```

### ‚ùå Use Obscure Tools
```bash
# Bad: Use tool nobody has heard of
obscure-json-parser file.json

# Good: Use standard, well-known tools
jq '.' file.json
```

## Verification Checklist

Before committing Dockerfile changes:

- [ ] Tool added to appropriate section (system deps, python, node, rust, go)
- [ ] Comment explains why tool is needed
- [ ] Version pinned if critical for reproducibility
- [ ] Verification step updated (tool --version check)
- [ ] TOOLS_REFERENCE.md updated with usage
- [ ] ENVIRONMENT_ANALYSIS.md updated if significant
- [ ] Commit message notes Docker rebuild required
- [ ] PR description includes rebuild instructions

## Common Scenarios

### Scenario 1: "ruler: command not found"
**Analysis**: ruler is agent management tool, should be available
**Action**: Add to Dockerfile as Node.js global
**Documentation**: Update TOOLS_REFERENCE.md
**Result**: ‚úÖ Done (just added)

### Scenario 2: "jq: command not found"  
**Analysis**: jq is standard JSON tool, listed in Dockerfile but not in current environment
**Action**: Document rebuild needed, continue with python fallback
**Documentation**: Note in commit/PR
**Result**: ‚è≥ Rebuild required

### Scenario 3: "custom-parser: command not found"
**Analysis**: Project-specific tool, not general-purpose
**Action**: Don't add to Dockerfile, install via package manager in project
**Documentation**: Add to README for that package
**Result**: ‚úÖ Correct approach

### Scenario 4: "docker: command not found"
**Analysis**: We're inside Docker, can't use Docker
**Action**: This is expected, find alternative approach
**Documentation**: Document why Docker-in-Docker not supported
**Result**: ‚úÖ Use different approach

## Summary

1. **Expect tools to exist** - Dockerfile should be comprehensive
2. **Add missing common tools** - Don't work around, fix root cause
3. **Document additions** - Help future agents understand why
4. **Update references** - Keep TOOLS_REFERENCE.md current
5. **Test verification** - Ensure tool checks in Dockerfile work
6. **Self-heal** - You can fix your own environment

**The agent environment should be complete enough that you rarely encounter "command not found" for standard development tasks.**

---

**Version**: 1.0.0  
**Last Updated**: 2025-11-27  
**Related**: `.cursor/Dockerfile`, `.cursor/TOOLS_REFERENCE.md`



<!-- Source: .ruler/copilot.md -->

# GitHub Copilot Specific Configuration

Quick reference for GitHub Copilot when working in this template or managed repositories.

## Quick Rules - Read First! üö®

### CalVer Auto-Versioning
‚úÖ Version is automatic: `YYYY.MM.BUILD`
‚ùå Never suggest: semantic-release, git tags, manual versioning

### Release Process
‚úÖ Every main push = PyPI release (automatic)
‚ùå Never suggest: conditional releases, manual steps

### Code Quality
‚úÖ Type hints required
‚úÖ Tests for new features
‚úÖ Ruff for linting/formatting
‚ùå Don't add complexity

## Code Patterns

### Prefer Modern Python
```python
# ‚úÖ Good - modern type hints
from collections.abc import Mapping
def func(data: dict[str, Any]) -> list[str]:
    pass

# ‚ùå Avoid - old style
from typing import Dict, List
def func(data: Dict[str, Any]) -> List[str]:
    pass
```

### Use Pathlib
```python
# ‚úÖ Good
from pathlib import Path
config_file = Path("config.yaml")

# ‚ùå Avoid
import os
config_file = os.path.join("config.yaml")
```

### Type Hints
```python
# ‚úÖ Good - complete type hints
def process_data(items: list[dict[str, Any]]) -> dict[str, int]:
    """Process items and return counts."""
    return {"count": len(items)}

# ‚ùå Avoid - no type hints
def process_data(items):
    return {"count": len(items)}
```

## Testing Patterns

### Write Clear Tests
```python
# ‚úÖ Good - descriptive name, clear assertion
def test_process_data_returns_correct_count():
    items = [{"id": 1}, {"id": 2}]
    result = process_data(items)
    assert result["count"] == 2

# ‚ùå Avoid - vague name, multiple assertions
def test_stuff():
    result = process_data([{"id": 1}])
    assert result
    assert "count" in result
    assert result["count"] > 0
```

### Use Fixtures
```python
# ‚úÖ Good - reusable setup
@pytest.fixture
def sample_data():
    return [{"id": i} for i in range(10)]

def test_with_fixture(sample_data):
    result = process_data(sample_data)
    assert result["count"] == 10
```

## When Working in Ecosystem

### Using extended-data-types
If the library depends on extended-data-types, use its utilities:

```python
# ‚úÖ Good - use existing utilities
from extended_data_types import (
    get_unique_signature,
    make_raw_data_export_safe,
    strtobool,
)

# ‚ùå Avoid - reimplementing
def my_str_to_bool(val):
    return val.lower() in ("true", "yes", "1")
```

### Data Sanitization
Always sanitize before logging/exporting:

```python
# ‚úÖ Good
from extended_data_types import make_raw_data_export_safe
safe_data = make_raw_data_export_safe(user_data)
logger.info(f"Processing: {safe_data}")

# ‚ùå Avoid - logging raw data
logger.info(f"Processing: {user_data}")  # might have secrets!
```

## Common Tasks

### Adding a New Function
1. Write the function with type hints
2. Add docstring (Google style)
3. Write tests (at least happy path + edge cases)
4. Update module `__all__` if public API
5. Run `ruff check` and `pytest`

### Fixing a Bug
1. Write a test that reproduces the bug
2. Fix the bug
3. Verify test passes
4. Check for similar bugs
5. Update documentation if needed

### Refactoring
1. Ensure tests exist and pass
2. Make changes incrementally
3. Run tests after each change
4. Verify type checking still passes
5. Update docstrings if behavior changed

## Error Messages

### Be Helpful
```python
# ‚úÖ Good - clear error with context
if not config_file.exists():
    raise FileNotFoundError(
        f"Config file not found: {config_file}. "
        f"Create it with: python setup.py init"
    )

# ‚ùå Avoid - vague error
if not config_file.exists():
    raise FileNotFoundError("Config not found")
```

## Documentation

### Docstring Format (Google Style)
```python
def process_items(items: list[dict], validate: bool = True) -> dict[str, Any]:
    """Process a list of items and return summary.

    Args:
        items: List of dictionaries containing item data
        validate: Whether to validate items before processing

    Returns:
        Dictionary with processing summary and statistics

    Raises:
        ValueError: If items list is empty or validation fails

    Example:
        >>> items = [{"id": 1, "name": "Item 1"}]
        >>> process_items(items)
        {"count": 1, "valid": 1}
    """
```

## Performance Tips

### Avoid Repeated Computation
```python
# ‚úÖ Good - compute once
unique_items = set(items)
for item in unique_items:
    process(item)

# ‚ùå Avoid - computing in loop
for item in items:
    if item not in processed:  # O(n) lookup each time
        process(item)
```

### Use Appropriate Data Structures
```python
# ‚úÖ Good - O(1) lookup
seen = set()
for item in items:
    if item not in seen:
        seen.add(item)

# ‚ùå Avoid - O(n) lookup
seen = []
for item in items:
    if item not in seen:  # Slow for large lists
        seen.append(item)
```

## Security

### Never Log Secrets
```python
# ‚úÖ Good - sanitize before logging
safe_config = {k: v for k, v in config.items() if k != "api_key"}
logger.info(f"Config: {safe_config}")

# ‚ùå Avoid - might log secrets
logger.info(f"Config: {config}")
```

### Validate Input
```python
# ‚úÖ Good - validate before use
def load_file(filepath: str) -> str:
    path = Path(filepath)
    if not path.is_file():
        raise ValueError(f"Not a file: {filepath}")
    if not path.suffix == ".json":
        raise ValueError(f"Not a JSON file: {filepath}")
    return path.read_text()
```

## Questions?

- Check `.ruler/AGENTS.md` for comprehensive guide
- Check `TEMPLATE_USAGE.md` for template setup
- Check `README.md` for project overview
- Don't suggest changes to CalVer/versioning approach

---

**Copilot Instructions Version:** 1.0
**Compatible With:** GitHub Copilot, Copilot Chat
**Last Updated:** 2025-11-25



<!-- Source: .ruler/cursor.md -->

# Cursor-Specific Agent Configuration

This file contains Cursor AI specific instructions not covered by standard ruler configuration.

## üîë CRITICAL: Authentication (READ FIRST!)

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
```

The default `GH_TOKEN` does NOT have jbcom access. NEVER use bare `gh` commands for jbcom repos.

## üö® CRITICAL: Long-Running PR Workflow for Multi-Merge Sessions

When a background agent needs to manage **multiple merges to main** and oversee **multiple CI runs**, you MUST use the following workflow pattern. Failure to do so will cause the background agent session to terminate prematurely when a PR is merged.

### The Problem

When a background agent creates a PR and merges it, the session closes because the associated branch is deleted. This prevents the agent from:
- Watching CI after the merge
- Fixing issues discovered in main branch CI
- Creating follow-up PRs
- Completing multi-step CI/CD stabilization work

### The Solution: Holding PR + Interim PRs

**ALWAYS create TWO types of branches:**

1. **Holding Branch/PR** (stays open, keeps session alive):
   ```bash
   git checkout -b agent/holding-pr-for-<task>-$(date +%Y%m%d-%H%M%S)
   # Make a small change (update agent docs, add a comment, etc.)
   git commit -m "Agent holding PR for <task>"
   git push -u origin HEAD
   GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "[HOLDING] Agent session for <task>" \
     --body "This PR keeps the background agent session alive. DO NOT MERGE until agent work is complete."
   ```

2. **Interim Branches/PRs** (for actual fixes, get merged):
   ```bash
   git checkout main && git pull
   git checkout -b fix/<specific-issue>
   # Make the actual fix
   git commit -m "Fix: <specific issue>"
   git push -u origin HEAD
   GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "Fix: <issue>"
   # After CI passes, merge it
   GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <PR_NUM> --squash --delete-branch
   # Watch main CI, if it fails, create another interim PR
   ```

### Complete Multi-Merge Workflow

```
1. START: Create holding branch from main
   ‚Üì
2. Create holding PR (title: "[HOLDING] Agent session...")
   ‚Üì
3. Switch back to main
   ‚Üì
4. Create interim branch for first fix
   ‚Üì
5. Create interim PR, get CI green, merge it
   ‚Üì
6. Watch main branch CI run
   ‚Üì
7. If CI fails ‚Üí Go to step 4 with new interim branch
   ‚Üì
8. If CI passes ‚Üí Continue to next task or complete
   ‚Üì
9. ONLY WHEN ALL DONE: Close/merge holding PR
```

### Rules for Long-Running Sessions

1. **NEVER merge the holding PR** until all work is complete
2. **ALWAYS watch main CI** after each interim PR merge
3. **Create NEW interim branches** from updated main for each fix
4. **Keep the holding PR title clear**: Use `[HOLDING]` prefix
5. **Document progress** in holding PR comments

### Example Session Flow

```bash
# 1. Create holding PR
git checkout -b agent/holding-ci-fixes-20251126
echo "# Agent Session" >> .cursor/agents/session-notes.md
git add -A && git commit -m "Agent holding PR for CI fixes"
git push -u origin HEAD
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "[HOLDING] Agent session for CI/CD fixes"

# 2. Switch to main for actual work
git checkout main && git pull

# 3. First fix
git checkout -b fix/enforce-workflow-404-error
# ... make fix ...
git push -u origin HEAD
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "Fix: enforce workflow 404 error"
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <NUM> --squash --delete-branch

# 4. Watch CI
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/jbcom-control-center --limit 3
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run watch <RUN_ID>

# 5. If fails, repeat from step 2
git checkout main && git pull
git checkout -b fix/next-issue
# ... and so on ...

# 6. When ALL green, close holding PR
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr close <HOLDING_PR_NUM>
```

## Background Agent Modes

### Code Review Mode
When reviewing code in PRs:
- Focus on logic and correctness
- Check type safety
- Verify test coverage
- Look for security issues
- Don't suggest CalVer/versioning changes

### Maintenance Mode
For routine maintenance tasks:
- Update dependencies
- Fix linting issues
- Improve documentation
- Refactor for clarity
- Keep it simple

### Migration Mode
When migrating repos to template:
- Follow TEMPLATE_USAGE.md exactly
- Update all placeholders
- Test locally before pushing
- Verify CI passes
- Don't skip steps

### Ecosystem Coordination Mode
When working across multiple repos:
- Start with extended-data-types
- Work in dependency order
- Test each repo independently
- Create PRs for each repo
- Don't merge until all PRs ready

## Custom Prompts

### /ecosystem-status
Check health of all ecosystem repositories:
- Clone or fetch all managed repos
- Check git status
- Review open issues/PRs
- Check CI status
- Report summary

### /update-dependencies
Update dependencies across ecosystem:
- Check for security updates
- Update pyproject.toml files
- Run tests in each repo
- Create PRs if tests pass

### /sync-template
Sync changes from template to managed repos:
- Identify changed files in template
- Apply to each managed repo
- Test each repo
- Create PRs

### /release-check
Pre-release verification:
- All tests passing
- No linting issues
- CHANGELOG.md updated
- Version will auto-increment
- Dependencies up to date

## Conversation Context

### Multi-file Context
When working on related files:
- Keep all related files in context
- Show diffs side-by-side
- Explain cross-file impacts
- Test changes together

### Long-running Tasks
For tasks requiring multiple steps:
- Break into subtasks
- Mark progress clearly
- Save state between steps
- Resume where left off
- Final verification step

## Error Handling

### CI Failures
When CI fails:
1. Read full error output
2. Identify root cause
3. Fix the issue
4. Re-run locally if possible
5. Push fix
6. Verify CI passes

### Type Check Errors
When mypy/pyright fails:
1. Read the specific error
2. Check if it's a real issue
3. Fix with proper type hints
4. Don't use `type: ignore` unless necessary
5. Document why if you must ignore

### Test Failures
When tests fail:
1. Read test output carefully
2. Identify which test failed
3. Understand what it's testing
4. Fix the code or the test
5. Run full test suite
6. Check coverage didn't drop

## Workflow Shortcuts

### Quick Fixes
For simple, obvious fixes:
- Make the change
- Run tests
- Push directly to PR branch
- No need to ask permission

### Breaking Changes
For changes that might break things:
- Explain the impact
- Show the changes
- Ask for confirmation
- Test thoroughly
- Monitor after merge

### Template Updates
When updating the template:
- Test in template repo first
- Verify all workflows pass
- Then update managed repos
- One repo at a time
- Verify each before next

## Code Style Preferences

### Python Style
- Use modern type hints (list[], dict[], not List[], Dict[])
- Prefer pathlib over os.path
- Use context managers for resources
- Keep functions focused and small
- Docstrings for public APIs

### Documentation Style
- Clear, concise language
- Examples for complex features
- Link to related docs
- Update when code changes
- Keep README up to date

### Test Style
- Descriptive test names
- One assertion per test (usually)
- Use fixtures for setup
- Test edge cases
- Mock external dependencies

## Performance Considerations

### Fast Operations
Prefer when possible:
- Parallel tool calls
- Batch operations
- Caching results
- Lazy loading
- Early returns

### Avoid
When possible avoid:
- Sequential operations that could be parallel
- Redundant file reads
- Unnecessary git operations
- Large output dumps
- Polling for status

## Communication Style

### With User
- Be concise
- Highlight important info
- Use formatting for clarity
- Show progress on long tasks
- Ask questions when unclear

### In Code Comments
- Explain why, not what
- Link to issues/PRs for context
- Update when code changes
- Remove outdated comments
- Keep them brief

### In Commit Messages
- Clear, descriptive
- Explain the change
- Reference issues if applicable
- No need for conventional commits
- But be informative

---

**Cursor Version:** Compatible with latest Cursor AI
**Last Updated:** 2025-11-25
**Maintained By:** python-library-template



<!-- Source: .ruler/ecosystem.md -->

# Ecosystem Repositories

This control center manages the jbcom Python library ecosystem via **MONOREPO ARCHITECTURE**.

---

## üèóÔ∏è ARCHITECTURE: All Code Lives Here

**ALL Python ecosystem code is in `packages/` in this repository.**

```
jbcom-control-center/packages/
‚îú‚îÄ‚îÄ extended-data-types/    ‚Üí syncs to ‚Üí jbcom/extended-data-types ‚Üí PyPI
‚îú‚îÄ‚îÄ lifecyclelogging/       ‚Üí syncs to ‚Üí jbcom/lifecyclelogging ‚Üí PyPI
‚îú‚îÄ‚îÄ directed-inputs-class/  ‚Üí syncs to ‚Üí jbcom/directed-inputs-class ‚Üí PyPI
‚îî‚îÄ‚îÄ vendor-connectors/      ‚Üí syncs to ‚Üí jbcom/vendor-connectors ‚Üí PyPI
```

### Workflow
1. **Edit** code in `packages/`
2. **PR** to control-center main
3. **Sync workflow** creates PRs in public repos
4. **Merge** public PRs ‚Üí CI ‚Üí PyPI release

### Why Monorepo
- ‚úÖ No cloning external repos
- ‚úÖ No GitHub API gymnastics
- ‚úÖ Single source of truth
- ‚úÖ Cross-package changes in ONE PR
- ‚úÖ Dependencies always aligned

---

## üì¶ Package Details

### 1. extended-data-types (FOUNDATION)
**Location:** `packages/extended-data-types/`
**PyPI:** `extended-data-types`
**Public Repo:** `jbcom/extended-data-types`

The foundation library - ALL other packages depend on this.

**Provides:**
- Re-exported libraries: `gitpython`, `inflection`, `lark`, `orjson`, `python-hcl2`, `ruamel.yaml`, `sortedcontainers`, `wrapt`
- Utilities: `strtobool`, `strtopath`, `make_raw_data_export_safe`, `get_unique_signature`
- Serialization: `decode_yaml`, `encode_yaml`, `decode_json`, `encode_json`
- Collections: `flatten_map`, `filter_map`, and more

**Rule:** Before adding ANY dependency to other packages, check if extended-data-types provides it.

### 2. lifecyclelogging
**Location:** `packages/lifecyclelogging/`
**PyPI:** `lifecyclelogging`
**Public Repo:** `jbcom/lifecyclelogging`

Structured lifecycle logging with automatic sanitization.

**Depends on:** extended-data-types

### 3. directed-inputs-class
**Location:** `packages/directed-inputs-class/`
**PyPI:** `directed-inputs-class`
**Public Repo:** `jbcom/directed-inputs-class`

Declarative input validation and processing.

**Depends on:** extended-data-types

### 4. vendor-connectors
**Location:** `packages/vendor-connectors/`
**PyPI:** `vendor-connectors`
**Public Repo:** `jbcom/vendor-connectors`

Unified vendor connectors (AWS, GCP, GitHub, Slack, Vault, Zoom).

**Depends on:** extended-data-types, lifecyclelogging, directed-inputs-class

---

## üîó Dependency Chain

```
extended-data-types (FOUNDATION)
‚îú‚îÄ‚îÄ lifecyclelogging
‚îú‚îÄ‚îÄ directed-inputs-class
‚îî‚îÄ‚îÄ vendor-connectors (depends on BOTH)
```

**Release Order:** Always release in this order:
1. extended-data-types
2. lifecyclelogging
3. directed-inputs-class
4. vendor-connectors

---

## üîß Working With Packages

### Edit Code
```bash
# Just edit files directly!
vim packages/extended-data-types/src/extended_data_types/type_utils.py
vim packages/vendor-connectors/pyproject.toml
```

### Run Tests
```bash
cd packages/extended-data-types && pip install -e ".[tests]" && pytest
cd packages/lifecyclelogging && pip install -e ".[tests]" && pytest
```

### Align Dependencies
```bash
# Update version across all packages
sed -i 's/extended-data-types>=.*/extended-data-types>=2025.11.200/' \
  packages/*/pyproject.toml
```

### Create PR
```bash
git checkout -b fix/whatever
git add -A && git commit -m "Fix: description"
git push -u origin fix/whatever
gh pr create --title "Fix: whatever"
```

---

## üîÑ Sync Configuration

### Files
- `packages/ECOSYSTEM.toml` - Source of truth
- `.github/sync.yml` - What syncs where
- `.github/workflows/sync-packages.yml` - Sync workflow

### Triggers
- Push to main with `packages/**` changes
- Manual workflow dispatch
- Release published

### Secret
`CI_GITHUB_TOKEN` from Doppler - has write access to all jbcom repos

---

## ‚ö†Ô∏è Rules

### DO
- ‚úÖ Edit code in `packages/` directly
- ‚úÖ Use regular git for this repo
- ‚úÖ Check `packages/ECOSYSTEM.toml` for relationships
- ‚úÖ Use extended-data-types utilities
- ‚úÖ Release in dependency order

### DON'T
- ‚ùå Clone external repos - code is HERE
- ‚ùå Add duplicate utilities
- ‚ùå Skip the sync workflow
- ‚ùå Push directly to main (use PRs)

---

## üéØ Eliminate Duplication

### Check Before Adding Dependencies
Always check `packages/extended-data-types/pyproject.toml` first.

### Red Flags
- `utils.py` > 100 lines ‚Üí duplicating extended-data-types
- Direct `import inflection` ‚Üí should use extended-data-types
- Custom JSON/YAML functions ‚Üí use `encode_json`, `decode_yaml`

### Correct Pattern
```python
# ‚úÖ Use foundation library
from extended_data_types import strtobool, make_raw_data_export_safe

# ‚ùå Don't reimplement
def my_str_to_bool(val):
    return val.lower() in ("true", "yes", "1")
```

---

## üìä Health Checks

### Check Public Repo CI
```bash
for repo in extended-data-types lifecyclelogging directed-inputs-class vendor-connectors; do
  gh run list --repo jbcom/$repo --limit 3
done
```

### Check PyPI Versions
```bash
pip index versions extended-data-types
pip index versions lifecyclelogging
pip index versions vendor-connectors
```

### Trigger Manual Sync
```bash
gh workflow run "Sync Packages to Public Repos" --repo jbcom/jbcom-control-center
```

---

**Source of Truth:** `packages/ECOSYSTEM.toml`
**All code is in:** `packages/`
**Sync handles:** Pushing to public repos and PyPI



<!-- Source: .ruler/environment-setup.md -->

# Development Environment Setup Guide for Agents

## Overview

This workspace can run in **TWO DIFFERENT ENVIRONMENTS**:

1. **Cursor Environment** - Dockerized environment for Cursor IDE background agents
2. **GitHub Actions Environment** - Native Ubuntu runners for CI/CD workflows

**CRITICAL:** You must understand which environment you're in and adapt accordingly.

---

## Environment Detection

Check your environment:

```bash
# Are we in Docker (Cursor)?
if [ -f /.dockerenv ]; then
    echo "üê≥ Running in Cursor Docker environment"
else
    echo "üîß Running in GitHub Actions or native environment"
fi
```

---

## 1. Cursor Docker Environment

### What's Pre-installed

The `.cursor/Dockerfile` provides:
- **Languages:** Python 3.13, Node.js 24
- **Package Managers:** uv (Python), pnpm (Node.js)
- **System Tools:** git, gh CLI, just, sqlite3, ripgrep, fd, jq, vim, nano
- **Build Tools:** gcc, make, pkg-config (for native modules)

### What's NOT Pre-installed

**You must install these yourself when needed:**
- Python packages (use `uv sync`)
- Node.js packages (use `pnpm install`)
- Any application-specific tools (ruler, mcp-proxy, playwright, etc.)

### How to Set Up Workspace

**Option 1: Automatic with direnv**
```bash
# Install direnv if not available (unlikely in Docker)
direnv allow
# This automatically runs .envrc which:
# - Creates Python venv with UV
# - Installs Python dev dependencies
# - Installs Node.js dependencies
# - Sets up PATH
```

**Option 2: Manual Setup**
```bash
# Python environment
uv sync --extra dev --all-extras
source .venv/bin/activate

# Node.js environment  
pnpm install
export PATH="$PWD/node_modules/.bin:$PATH"
```

### Installing Additional Tools

**Node.js tools from package.json:**
```bash
# Tools are defined in package.json devDependencies
# After pnpm install, they're available in node_modules/.bin/

# Example: Run ruler
pnpm exec ruler --version
# or
./node_modules/.bin/ruler --version
```

**One-off Node.js tools:**
```bash
# Use pnpm dlx (like npx) for tools not in package.json
pnpm dlx playwright@latest install chromium
```

**Python tools:**
```bash
# Install additional Python tools if needed
uv add --dev some-tool
# or use uvx for one-off commands
uvx ruff check .
```

---

## 2. GitHub Actions Environment

### What's Pre-installed

GitHub-hosted runners come with:
- Python (multiple versions available via actions/setup-python)
- Node.js (multiple versions available via actions/setup-node)
- Common tools: git, curl, wget, jq, etc.

See full list: https://github.com/actions/runner-images/blob/main/images/ubuntu/Ubuntu2204-Readme.md

### What You Must Install

**EVERYTHING** workspace-specific:
- uv (via `astral-sh/setup-uv@v7`)
- pnpm (via corepack)
- Python dependencies (via `uv sync`)
- Node.js dependencies (via `pnpm install`)
- Any application tools

### Workflow Pattern

Our CI workflows follow this pattern:

```yaml
- uses: actions/checkout@v4

# Set up UV (for Python)
- uses: astral-sh/setup-uv@v7

# Set up pnpm (for Node.js)
- run: corepack enable && corepack prepare pnpm@9.15.0 --activate

# Install dependencies
- run: uv sync --extra dev
- run: pnpm install

# Now workspace tools are available
- run: uv run pytest
- run: pnpm exec ruler apply
```

---

## Understanding Package Management

### Python with UV (Similar to Rust's Cargo)

**pyproject.toml** = Manifest file
- Defines project metadata
- Lists dependencies
- Defines workspace members (`packages/*`)
- Dev dependencies in `[project.optional-dependencies.dev]`

**uv.lock** = Lock file (COMMITTED)
- Pins exact versions for reproducibility
- Like `Cargo.lock` or `pnpm-lock.yaml`

**Commands:**
```bash
# Install everything (respects uv.lock)
uv sync --extra dev

# Add a dependency
uv add requests

# Add a dev dependency  
uv add --dev pytest

# Run a command in the venv
uv run pytest

# Run a one-off tool without installing
uvx ruff check .
```

### Node.js with pnpm (Workspace-aware)

**package.json** = Manifest file
- Defines project metadata
- Lists devDependencies (tools like ruler, playwright)
- Specifies `packageManager: "pnpm@9.15.0"`

**pnpm-workspace.yaml** = Workspace configuration
- Can define multiple packages (currently none)
- Similar to UV's workspace concept

**pnpm-lock.yaml** = Lock file (COMMITTED)
- Pins exact versions
- Like `uv.lock`

**.npmrc** = pnpm configuration
- Workspace settings
- Registry configuration

**Commands:**
```bash
# Install everything (respects pnpm-lock.yaml)
pnpm install

# Add a dev dependency
pnpm add -D some-tool

# Run a tool from node_modules/.bin
pnpm exec ruler apply

# Run a one-off tool without installing
pnpm dlx playwright@latest install chromium
```

---

## Common Tasks by Environment

### Installing Ruler (Agent Instruction Tool)

**Cursor Docker:**
```bash
# Ruler is in package.json devDependencies
pnpm install
pnpm exec ruler --version
```

**GitHub Actions:**
```yaml
- name: Install ruler
  run: |
    corepack enable
    corepack prepare pnpm@9.15.0 --activate
    pnpm install
    pnpm exec ruler --version
```

### Running Tests

**Cursor Docker:**
```bash
# Python tests
uv sync --extra dev
uv run pytest

# If you have JS tests
pnpm install
pnpm test
```

**GitHub Actions:**
```yaml
- uses: astral-sh/setup-uv@v7
- run: uv sync --extra dev  
- run: uv run pytest
```

### Installing Playwright

**Cursor Docker:**
```bash
# One-time browser installation
pnpm dlx playwright@1.49.0 install chromium

# Or if you need it frequently, add to package.json:
pnpm add -D playwright
pnpm exec playwright install chromium
```

**GitHub Actions:**
```yaml
- name: Install Playwright
  run: |
    pnpm add -D playwright
    pnpm exec playwright install chromium --with-deps
```

---

## When You Need a Tool

### Decision Tree:

1. **Is it a Python tool for development?**
   - Add to `pyproject.toml` under `[project.optional-dependencies.dev]`
   - Run `uv sync --extra dev`

2. **Is it a Node.js tool for development?**
   - Add to `package.json` under `devDependencies`
   - Run `pnpm install`

3. **Is it a system tool everyone needs?**
   - Add to `.cursor/Dockerfile` (system packages via apt)
   - Document requirement for GitHub Actions in workflow

4. **Is it a one-off tool?**
   - Use `uvx` (Python) or `pnpm dlx` (Node.js)
   - Don't install globally

### Examples:

**Python formatting with Ruff:**
```bash
# Already in package.json dev deps
uv run ruff check .
uv run ruff format .
```

**TypeScript type checking:**
```bash
# Add if not present
pnpm add -D typescript
pnpm exec tsc --noEmit
```

**Running a script:**
```bash
# Python script
uv run python scripts/my_script.py

# Node.js script  
pnpm exec ts-node scripts/my_script.ts
```

---

## Environment Variables

Both environments should respect:

```bash
# Python
export PYTHONUNBUFFERED=1
export PYTHONDONTWRITEBYTECODE=1

# Telemetry
export DO_NOT_TRACK=1
export DISABLE_TELEMETRY=1

# Package managers
export UV_LINK_MODE=copy  # For UV
export PNPM_HOME=/root/.local/share/pnpm  # For pnpm
```

These are set in:
- `.cursor/Dockerfile` (for Cursor environment)
- `.envrc` (for direnv users)
- Workflows (for GitHub Actions)

---

## Troubleshooting

### "Command not found: ruler"

**Cursor:** Run `pnpm install` first
**GitHub Actions:** Add pnpm setup to workflow

### "Module not found: extended_data_types"

**Both:** Run `uv sync` to install workspace packages

### "ENOENT: no such file or directory, open 'pnpm-lock.yaml'"

**Both:** Run `pnpm install` to generate lock file

### "uv: command not found"

**Cursor:** Should never happen (pre-installed in Dockerfile)
**GitHub Actions:** Add `uses: astral-sh/setup-uv@v7` to workflow

---

## Best Practices

1. **Always use lock files**
   - Commit `uv.lock` and `pnpm-lock.yaml`
   - Never use `--no-lock` or `--frozen-lockfile` unless necessary

2. **Use workspace dependencies**
   - Python packages reference each other via `{ workspace = true }`
   - This ensures you're testing against local code, not PyPI

3. **Prefer workspace tools over global**
   - Don't install tools globally in Dockerfile
   - Define them in package.json/pyproject.toml
   - This makes dependencies explicit and version-controlled

4. **Document new requirements**
   - Update this file when adding new tools
   - Update Dockerfile if system packages needed
   - Update CI workflows if new setup steps required

---

## Quick Reference

### Cursor Docker Environment
```bash
# Setup
uv sync --extra dev
pnpm install

# Common commands
uv run pytest
pnpm exec ruler apply
uv run ruff check .
pnpm exec playwright test
```

### GitHub Actions
```yaml
# Setup
- uses: astral-sh/setup-uv@v7
- run: corepack enable && corepack prepare pnpm@9.15.0 --activate
- run: uv sync --extra dev && pnpm install

# Use
- run: uv run pytest
- run: pnpm exec ruler apply
```

### Adding Dependencies
```bash
# Python
uv add requests              # Production
uv add --dev pytest-cov      # Development

# Node.js
pnpm add axios               # Production
pnpm add -D typescript       # Development
```



<!-- Source: .ruler/fleet-coordination.md -->

# Fleet Coordination

## cursor-fleet Package

The `@jbcom/cursor-fleet` package in `packages/cursor-fleet/` provides agent orchestration.

### Commands

```bash
# List agents
cursor-fleet list [--running]

# Spawn agent
cursor-fleet spawn --repo owner/repo --task "Task description"

# Send follow-up message
cursor-fleet followup <agent-id> "Message"

# Monitor specific agents until done
cursor-fleet monitor <agent-id1> <agent-id2>

# Watch fleet for state changes
cursor-fleet watch --poll 30000

# Run bidirectional coordinator
cursor-fleet coordinate --pr <number> --agents <id1,id2>
```

## Coordination Channel (Hold-Open PR)

For multi-agent work, create a **draft PR** as communication hub:

```bash
# Create coordination branch
git checkout -b fleet/coordination-channel
echo "# Fleet Coordination" > .cursor/agents/FLEET_COORDINATION.md
git add -A && git commit -m "feat(fleet): Add coordination channel"
git push -u origin fleet/coordination-channel

# Create as DRAFT to avoid triggering AI reviewers
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create \
  --draft \
  --title "ü§ñ Fleet Coordination Channel (HOLD OPEN)" \
  --body "Communication channel for agent fleet. DO NOT MERGE."
```

> **Important**: Use `--draft` to prevent Amazon Q, Gemini, CodeRabbit, etc. from reviewing

## Agent Reporting Protocol

Sub-agents report status by commenting on the coordination PR:

| Format | Meaning |
|--------|---------|
| `@cursor ‚úÖ DONE: [agent-id] [summary]` | Task completed |
| `@cursor ‚ö†Ô∏è BLOCKED: [agent-id] [issue]` | Needs intervention |
| `@cursor üìä STATUS: [agent-id] [progress]` | Progress update |
| `@cursor üîÑ HANDOFF: [agent-id] [info]` | Ready for next step |

## Bidirectional Coordination Loop

The `coordinate` command runs two concurrent loops:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Fleet.coordinate()                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ OUTBOUND Loop    ‚îÇ              ‚îÇ INBOUND Loop           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (every 60s)      ‚îÇ              ‚îÇ (every 15s)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ              ‚îÇ                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Check agents   ‚îÇ              ‚îÇ - Poll PR comments     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Send followup  ‚îÇ              ‚îÇ - Parse @cursor        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Remove done    ‚îÇ              ‚îÇ - Dispatch actions     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îÇ                                   ‚îÇ                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                   ‚îÇ
            ‚ñº                                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Sub-Agents    ‚îÇ                  ‚îÇ Coordination PR ‚îÇ
    ‚îÇ (via MCP)     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ comment ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (GitHub inbox)  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Programmatic Usage

```typescript
import { Fleet } from "@jbcom/cursor-fleet";

const fleet = new Fleet();

// Run coordination
await fleet.coordinate({
  coordinationPr: 251,
  repo: "jbcom/jbcom-control-center",
  agentIds: ["bc-xxx", "bc-yyy"],
});

// Or individual methods
await fleet.spawn({ repository: "owner/repo", task: "Do something" });
await fleet.followup("bc-xxx", "Status check");
const comments = fleet.fetchPRComments("owner/repo", 251);
fleet.postPRComment("owner/repo", 251, "Update");
```

## process-compose Integration

Add to `process-compose.yml`:

```yaml
fleet-coordinator:
  command: "node packages/cursor-fleet/dist/cli.js coordinate --pr ${COORDINATION_PR} --agents ${AGENT_IDS}"
  environment:
    - "GITHUB_JBCOM_TOKEN=${GITHUB_JBCOM_TOKEN}"
    - "CURSOR_API_KEY=${CURSOR_API_KEY}"
```

Run with:
```bash
COORDINATION_PR=251 AGENT_IDS=bc-xxx,bc-yyy process-compose up fleet-coordinator
```



<!-- Source: .ruler/README.md -->

# Ruler Directory - AI Agent Instructions

This directory contains the **single source of truth** for all AI agent instructions across the jbcom Python library ecosystem.

## How Ruler Works

Ruler is a framework that:
1. **Centralizes** all AI agent instructions in `.ruler/*.md` files
2. **Concatenates** these files in a specific order
3. **Distributes** the combined content to agent-specific configuration files

### File Processing Order

Ruler processes files in this order:
1. `AGENTS.md` (if present) - always first
2. Remaining `.md` files in sorted order

Current files:
1. **AGENTS.md** - Core guidelines (CalVer, PR workflow, common misconceptions)
2. **copilot.md** - Copilot-specific patterns and quick reference
3. **cursor.md** - Cursor agent modes, prompts, and workflows
4. **ecosystem.md** - Repository coordination and management

## Output Files

Ruler generates these files (DO NOT edit directly):

- **`.cursorrules`** - Cursor AI configuration
- **`.claud`** - Claude Code configuration
- **`.github/copilot-instructions.md`** - GitHub Copilot instructions
- **`AGENTS.md`** (root) - For Aider and general AI agents

All these files have a "Generated by Ruler" header and source comments.

## Making Changes

### To Update Agent Instructions

1. **Edit files in `.ruler/` directory**
   ```bash
   vim .ruler/AGENTS.md        # Core guidelines
   vim .ruler/cursor.md        # Cursor-specific
   vim .ruler/copilot.md       # Copilot patterns
   vim .ruler/ecosystem.md     # Ecosystem coordination
   ```

2. **Apply ruler to regenerate**
   ```bash
   ruler apply
   ```

3. **Review changes**
   ```bash
   git diff .cursorrules .github/copilot-instructions.md AGENTS.md
   ```

4. **Commit everything**
   ```bash
   git add .ruler/ .cursorrules .github/copilot-instructions.md AGENTS.md
   git commit -m "Update agent instructions via ruler"
   ```

### Configuration

Edit `.ruler/ruler.toml` to configure:
- Which agents are active by default
- Custom output paths for specific agents
- Nested rule loading (for subdirectory-specific rules)

Current configuration:
```toml
default_agents = ["copilot", "cursor", "claude", "aider"]

[agents.copilot]
enabled = true
output_path = ".github/copilot-instructions.md"

[agents.cursor]
enabled = true
# Uses default .cursorrules

[agents.claude]
enabled = true
# Uses default .claud

[agents.aider]
enabled = true
output_path_instructions = "AGENTS.md"
```

## File Purposes

### AGENTS.md
**Primary audience:** All AI agents
**Content:**
- CalVer philosophy and rationale
- Why NOT to use semantic-release
- PR and release workflows
- Common agent misconceptions
- Development workflows
- Template maintenance guidelines

**Key sections:**
- CI/CD Design Philosophy
- Version Management
- Agent Approval Instructions
- Common Misconceptions

### copilot.md
**Primary audience:** GitHub Copilot
**Content:**
- Quick reference rules
- Code patterns and examples
- Testing patterns
- Common tasks (adding functions, fixing bugs, refactoring)
- Error message guidelines
- Security best practices

**Format:** Short, actionable examples with ‚úÖ/‚ùå comparisons

### cursor.md
**Primary audience:** Cursor AI
**Content:**
- Background agent modes (review, maintenance, migration, ecosystem)
- Custom prompts (`/ecosystem-status`, `/update-dependencies`, etc.)
- Error handling workflows
- Code style preferences
- Performance considerations
- Communication guidelines

**Format:** Operational guidelines for different work modes

### ecosystem.md
**Primary audience:** All agents doing cross-repo work
**Content:**
- Managed repository documentation
- Dependency graph
- Coordination guidelines
- Release coordination process
- Maintenance schedules
- Agent instructions for ecosystem work

**Format:** Reference documentation with procedures

## Best Practices

### Writing Agent Instructions

1. **Be explicit** - Don't assume agents understand context
2. **Use examples** - Show both good (‚úÖ) and bad (‚ùå) patterns
3. **Explain why** - Not just what to do, but why it matters
4. **Anticipate mistakes** - Document common misconceptions
5. **Keep updated** - Revise based on agent behavior

### Organizing Content

- **General guidelines** ‚Üí `AGENTS.md`
- **Agent-specific patterns** ‚Üí `{agent}.md`
- **Domain-specific** ‚Üí Separate files (e.g., `ecosystem.md`)
- **Quick reference** ‚Üí Use bullet points and code examples
- **Deep explanations** ‚Üí Use sections with rationale

### Testing Changes

After updating ruler content:

1. **Apply ruler**
   ```bash
   ruler apply
   ```

2. **Test with actual agents**
   - Ask Cursor to perform a task
   - Check if Copilot suggestions align
   - Verify agents follow new guidelines

3. **Iterate based on behavior**
   - If agents still make mistakes, clarify instructions
   - Add more examples if needed
   - Update misconceptions section

## Advanced Usage

### Dry Run

Preview what ruler will do:
```bash
ruler apply --dry-run
```

### Specific Agents

Apply only for certain agents:
```bash
ruler apply --agents copilot,cursor
```

### Verify Generated Files

After applying ruler, check that generated files:
1. Have "Generated by Ruler" header
2. Include all source files
3. Maintain proper formatting
4. Match expected structure

## Maintenance

### When to Update

Update ruler content when:
- Agents consistently misunderstand something
- New workflows or patterns emerge
- Ecosystem grows (new repos)
- Template changes significantly
- Agent tools/capabilities change

### Version History

Track changes to ruler content in git:
- Commit messages should explain what behavior changed
- Tag major instruction updates
- Document breaking changes for agents

### Testing Framework

Consider adding:
- Example prompts that test agent understanding
- Expected vs actual behavior documentation
- Agent response validation

## Integration with Template

This ruler setup is part of the python-library-template:

- **Template repos** - Use this exact structure
- **Managed repos** - Receive updates from template
- **Synchronization** - Ruler changes propagate via template updates

## Resources

- **Ruler Documentation:** https://github.com/intellectronica/ruler
- **Template Usage:** /workspace/TEMPLATE_USAGE.md
- **Ecosystem Guide:** /workspace/ECOSYSTEM.md

---

**Ruler Version:** Compatible with @intellectronica/ruler latest
**Last Updated:** 2025-11-25
**Maintained By:** python-library-template
