<!-- Generated by Ruler -->


<!-- Source: .ruler/AGENTS.md -->

# AI Agent Guidelines for Python Library Template (jbcom ecosystem)

**This is the DEFINITIVE Python library template** for the jbcom ecosystem. All configuration, workflows, and agent instructions here represent the consolidated best practices from multiple production deployments.

## üîë CRITICAL: Authentication (READ FIRST!)

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh workflow run "Release" --repo jbcom/jbcom-control-center
```

### Token Reference:
- **GITHUB_JBCOM_TOKEN** - Use for ALL jbcom repo operations (PRs, merges, workflow triggers)
- **CI_GITHUB_TOKEN** - Used by GitHub Actions workflows (in repo secrets)
- **PYPI_TOKEN** - Used by release workflow for PyPI publishing (in repo secrets)

### ‚ö†Ô∏è NEVER FORGET:
The default `GH_TOKEN` does NOT have access to jbcom repos. You MUST prefix with `GH_TOKEN="$GITHUB_JBCOM_TOKEN"` for EVERY `gh` command targeting jbcom repos.

## üéØ PURPOSE: Agentic Template Repository

This template is designed for:
1. **Human developers** starting new Python libraries
2. **AI coding assistants** (Cursor, Codex, Copilot, Gemini) helping maintain the ecosystem
3. **Background agents** performing automated maintenance tasks

### Template Usage

When creating a new library from this template:
1. Update `pyproject.toml` with your project name and details
2. Replace `${REPO_NAME}` in documentation with your actual repo name
3. Copy `.github/scripts/set_version.py` as-is (it auto-detects your package)
4. Copy `.github/workflows/ci.yml` and update PyPI project name
5. Run `ruler apply` to regenerate agent-specific instructions

## üö® CRITICAL: CI/CD Workflow Design Philosophy

### Our Simple Automated Release Workflow

**This repository uses CALENDAR VERSIONING with automatic PyPI releases**. Every push to main that passes tests gets released automatically.

This design has been battle-tested across:
- `extended-data-types` (foundational library, released 2025.11.164)
- `lifecyclelogging` (logging library)  
- `directed-inputs-class` (input processing)

### Key Design Decisions (DO NOT SUGGEST CHANGING THESE)

#### 1. **Calendar Versioning (CalVer) - No Manual Version Management**

‚úÖ **How It Works:**
- Version format: `YYYY.MM.BUILD_NUMBER`
- Example: `2025.11.42`
- **Month is NOT zero-padded** (project choice for brevity)
- Version is auto-generated using GitHub run number
- Script: `.github/scripts/set_version.py`

‚ùå **INCORRECT Agent Suggestion:**
> "You should manually manage versions in __init__.py"
> "Add semantic-release for version management"
> "Use git tags for versioning"
> "Zero-pad the month for consistency"

‚úÖ **CORRECT Understanding:**
- Version is AUTOMATICALLY updated on every main branch push
- No git tags needed or used
- No semantic analysis of commits needed
- No manual version bumps required
- Month padding is a project preference (we chose no padding)

#### 2. **Every Push to Main = PyPI Release**

‚úÖ **How It Works:**
```
Push to main branch
  ‚Üì
All tests pass
  ‚Üì
Auto-generate version (YYYY.MM.BUILD)
  ‚Üì
Build signed package
  ‚Üì
Publish to PyPI
  ‚Üì
DONE
```

‚ùå **INCORRECT Agent Suggestion:**
> "Only release when version changes"
> "Check if release is needed before publishing"
> "Use conditional logic to skip releases"

‚úÖ **CORRECT Understanding:**
- Every main branch push = new release
- No conditionals, no skipping
- Simple, predictable, automatic
- If code was merged to main, it should be released

#### 3. **No Git Tags, No GitHub Releases**

‚úÖ **What We Do:**
- Publish directly to PyPI
- Version in package metadata only
- PyPI is the source of truth for releases

‚ùå **What We Don't Do:**
- ‚ùå Create git tags
- ‚ùå Create GitHub releases  
- ‚ùå Manage changelog files automatically
- ‚ùå Commit version changes back to repo

#### 4. **Why This Approach?**

**Problems with semantic-release and tag-based versioning:**
- Complex setup and configuration
- Depends on commit message conventions
- Requires git tags and history analysis
- Can fail or skip releases unexpectedly
- Adds unnecessary complexity
- Multiple points of failure

**Benefits of CalVer + Auto-increment:**
- ‚úÖ Dead simple - minimal configuration
- ‚úÖ Always works - no analysis, no skipping
- ‚úÖ Predictable - every push = new version
- ‚úÖ No git pollution - no tags, no bot commits
- ‚úÖ Build number always increments
- ‚úÖ Fails loudly with clear error messages

## üìù Making Code Changes

### When Reviewing PRs

**DO:**
- ‚úÖ Review code quality and correctness
- ‚úÖ Check test coverage
- ‚úÖ Verify type hints
- ‚úÖ Suggest API improvements
- ‚úÖ Check for security issues
- ‚úÖ Verify dependencies are up to date

**DO NOT:**
- ‚ùå Suggest adding semantic-release
- ‚ùå Recommend manual version management
- ‚ùå Suggest git tag-based versioning
- ‚ùå Recommend complex release workflows
- ‚ùå Flag the simple auto-versioning as "incorrect"
- ‚ùå Suggest zero-padding months (unless project explicitly wants it)

### Understanding Version Management

```
GitHub Action Starts
  ‚Üì
Run Number: 42 (GitHub auto-increments)
  ‚Üì
Script generates: 2025.11.42
  ‚Üì
Updates __init__.py: __version__ = "2025.11.42"
  ‚Üì
Updates docs/conf.py: version = "2025.11.42" (if exists)
  ‚Üì
Package built with that version
  ‚Üì
Published to PyPI
```

**Version is:**
- ‚úÖ Generated automatically
- ‚úÖ Based on date + GitHub run number
- ‚úÖ Always incrementing
- ‚úÖ Written to __init__.py during build
- ‚úÖ Also written to docs/conf.py if it exists
- ‚ùå NOT committed back to git
- ‚ùå NOT based on git tags

### The set_version.py Script

Key features of the production-tested script:
- **Auto-detects** the package `__init__.py` in `src/`
- **Validates** that exactly one __version__ declaration exists
- **Uses regex** to match version assignment (handles quotes, spacing, etc.)
- **Fails loudly** with clear error if update doesn't work
- **Updates docs/conf.py** if it exists (optional)
- **Preserves indentation** and file formatting
- **Ruff-compliant** with proper noqa annotations

### Commit Message Format

**You can use any commit format you want.** We don't analyze commits for versioning.

Recommendations:
- Write clear, descriptive commits
- Explain what and why
- No specific format required
- Conventional commits are fine but not enforced

## ü§ñ For AI Agents: Approval & Override Instructions

### Background Agent Behavior

When you are operating as a **background agent** (autonomous, no user interaction):

**CRITICAL RULES:**
1. **DO NOT** create or merge PRs without explicit user approval
2. **DO NOT** push directly to main branch
3. **DO** create PRs and mark them as ready for review
4. **DO** run all CI checks and fix linting/test failures
5. **DO** respond to PR feedback and iterate
6. **WAIT** for human approval before merging

**EXCEPTION - When User Says:**
> "merge it", "go ahead and merge", "merge to main", "just push it"

Then you MAY:
- Merge PRs after CI passes
- Push directly to main if explicitly instructed
- Skip the usual "wait for approval" step

**HOW TO MERGE:**
```bash
gh pr merge <PR_NUMBER> --squash --delete-branch
```

### Interactive Agent Behavior

When you are in **interactive mode** (user is actively engaged):

**DEFAULT BEHAVIOR:**
- Ask for confirmation before major actions
- Present options and let user decide
- Show diffs and explain changes

**WHEN USER GETS FRUSTRATED:**
User signals like "just do it", "stop asking", "I don't care", "fucking do it":
- Switch to autonomous mode
- Make decisions based on best practices in this document
- Execute without asking for every little thing
- Still verify after major milestones

### PR Review Response

When PR feedback arrives (from Copilot, Gemini, human reviewers):

1. **READ the feedback carefully**
2. **CHECK** if feedback contradicts this document
3. **IF** feedback suggests semantic-release, git tags, or manual versioning:
   - Politely explain our CalVer approach
   - Reference this document
   - Do NOT implement the suggestion
4. **IF** feedback is about code quality, bugs, or improvements:
   - Implement the feedback
   - Push fixes
   - Verify CI passes

### Template Maintenance Tasks

As an agent maintaining this template:

**ALLOWED:**
- Update dependencies
- Fix security vulnerabilities
- Improve documentation clarity
- Add helpful examples
- Fix bugs in scripts or workflows

**NOT ALLOWED WITHOUT USER APPROVAL:**
- Change the versioning approach
- Modify CI workflow structure
- Remove or bypass safety checks
- Change the release process

## üîß Development Workflow

### Local Development

```bash
# Install dependencies
pip install -e ".[tests,typing,docs]"  # or use poetry/uv

# Run tests
pytest

# Run type checking
mypy src/  # or pyright

# Run linting
pre-commit run --all-files
```

### Creating PRs

1. Create a feature branch
2. Make your changes
3. Run tests locally
4. Create PR against `main`
5. CI will run automatically
6. Address any feedback
7. Merge to main when approved

### Releases (Fully Automated)

When PR is merged to main:
1. CI runs all checks
2. Auto-generates version: `YYYY.MM.BUILD`
3. Builds signed package with attestations
4. Publishes to PyPI
5. **DONE - that's it**

No manual steps, no tags, no conditionals, no complexity.

## üéØ Common Agent Misconceptions

### Misconception #1: "Missing version management"
**Agent says:** "You need to manually update __version__ before releases"
**Reality:** Version is auto-generated on every main branch push. Manual management not needed and will be overwritten.

### Misconception #2: "Should use semantic versioning"
**Agent says:** "Consider using semantic-release or conventional commits"
**Reality:** We intentionally use CalVer for simplicity. Every push gets a new version. This has been deployed successfully across multiple production libraries.

### Misconception #3: "Need git tags"
**Agent says:** "Add git tags for release tracking"
**Reality:** PyPI version history is our source of truth. No git tags needed. We tried this, it caused more problems than it solved.

### Misconception #4: "CalVer is wrong for libraries"
**Agent says:** "Libraries should use SemVer"
**Reality:** CalVer works fine for our ecosystem. Users pin versions anyway. Simplicity and reliability > convention. Our dependencies work with CalVer.

### Misconception #5: "Missing release conditions"
**Agent says:** "You should only release when changes are made"
**Reality:** Every main push is intentional. If it was merged, it should be released. Empty releases are fine and caught by PyPI anyway.

### Misconception #6: "Month should be zero-padded"
**Agent says:** "Use 2025.01.42 instead of 2025.1.42"
**Reality:** This is a project-specific choice. We chose no padding for brevity. CalVer allows both. Don't suggest changing it.

### Misconception #7: "Need to commit version back to git"
**Agent says:** "Version changes should be committed to the repository"
**Reality:** NO. Versions are ephemeral build artifacts. Committing them creates noise and potential conflicts. The script updates them during CI only.

## üìö Design Rationale

This workflow was created to solve REAL problems we encountered:

**Problems We Solved:**
- ‚úÖ No more failed releases due to missing tags
- ‚úÖ No more version conflicts between branches
- ‚úÖ No more "why didn't it release?" debugging sessions
- ‚úÖ No more complex semantic-release configuration issues
- ‚úÖ No more dependency on git history analysis
- ‚úÖ No more bot commits cluttering git history
- ‚úÖ No more release workflow that sometimes works, sometimes doesn't

**Benefits We Gained:**
- ‚úÖ Predictable: every main push = release
- ‚úÖ Simple: ~100 lines of Python for versioning
- ‚úÖ Reliable: no conditional logic to fail
- ‚úÖ Fast: no git history analysis overhead
- ‚úÖ Clean: no bot commits or tags in git
- ‚úÖ Debuggable: clear error messages when things fail
- ‚úÖ Testable: can run script locally with ease

## üß™ Testing the Workflow

### Local Testing

Test the versioning script locally:
```bash
export GITHUB_RUN_NUMBER=999
python .github/scripts/set_version.py

# Verify it updated the version
grep __version__ src/your_package/__init__.py
```

### CI Testing

Test in a PR:
1. Create a PR
2. Watch CI run
3. Verify all checks pass
4. Check that versioning step succeeds

### Release Testing

To test an actual release:
1. Merge a PR to main
2. Watch the CI run
3. Verify version was generated (check logs)
4. Verify package was built with correct version
5. Verify publish to PyPI succeeded
6. Install from PyPI and verify: `pip install your-package==2025.XX.YYY`

## ü§ù Getting Help

### For AI Agents

If you're an AI agent uncertain about a suggestion:
1. **Check this document first** - it's comprehensive
2. If it involves versioning or releases, **DO NOT suggest changes**
3. Focus on code quality, tests, and documentation
4. Trust that the release workflow is intentionally simple
5. When in doubt, ask the user instead of assuming

### For Human Developers

- This template has been tested in production
- Don't overthink it - it's designed to be simple
- If something seems weird, check this document first
- The versioning really is meant to be automatic
- Trust the process - it works

---

**Last Updated:** 2025-11-25
**Versioning:** CalVer (YYYY.MM.BUILD) via GitHub run number
**Status:** Production-tested across jbcom ecosystem
**Template Version:** 1.0.0



<!-- Source: .ruler/copilot.md -->

# GitHub Copilot Specific Configuration

Quick reference for GitHub Copilot when working in this template or managed repositories.

## Quick Rules - Read First! üö®

### CalVer Auto-Versioning
‚úÖ Version is automatic: `YYYY.MM.BUILD`
‚ùå Never suggest: semantic-release, git tags, manual versioning

### Release Process
‚úÖ Every main push = PyPI release (automatic)
‚ùå Never suggest: conditional releases, manual steps

### Code Quality
‚úÖ Type hints required
‚úÖ Tests for new features
‚úÖ Ruff for linting/formatting
‚ùå Don't add complexity

## Code Patterns

### Prefer Modern Python
```python
# ‚úÖ Good - modern type hints
from collections.abc import Mapping
def func(data: dict[str, Any]) -> list[str]:
    pass

# ‚ùå Avoid - old style
from typing import Dict, List
def func(data: Dict[str, Any]) -> List[str]:
    pass
```

### Use Pathlib
```python
# ‚úÖ Good
from pathlib import Path
config_file = Path("config.yaml")

# ‚ùå Avoid
import os
config_file = os.path.join("config.yaml")
```

### Type Hints
```python
# ‚úÖ Good - complete type hints
def process_data(items: list[dict[str, Any]]) -> dict[str, int]:
    """Process items and return counts."""
    return {"count": len(items)}

# ‚ùå Avoid - no type hints
def process_data(items):
    return {"count": len(items)}
```

## Testing Patterns

### Write Clear Tests
```python
# ‚úÖ Good - descriptive name, clear assertion
def test_process_data_returns_correct_count():
    items = [{"id": 1}, {"id": 2}]
    result = process_data(items)
    assert result["count"] == 2

# ‚ùå Avoid - vague name, multiple assertions
def test_stuff():
    result = process_data([{"id": 1}])
    assert result
    assert "count" in result
    assert result["count"] > 0
```

### Use Fixtures
```python
# ‚úÖ Good - reusable setup
@pytest.fixture
def sample_data():
    return [{"id": i} for i in range(10)]

def test_with_fixture(sample_data):
    result = process_data(sample_data)
    assert result["count"] == 10
```

## When Working in Ecosystem

### Using extended-data-types
If the library depends on extended-data-types, use its utilities:

```python
# ‚úÖ Good - use existing utilities
from extended_data_types import (
    get_unique_signature,
    make_raw_data_export_safe,
    strtobool,
)

# ‚ùå Avoid - reimplementing
def my_str_to_bool(val):
    return val.lower() in ("true", "yes", "1")
```

### Data Sanitization
Always sanitize before logging/exporting:

```python
# ‚úÖ Good
from extended_data_types import make_raw_data_export_safe
safe_data = make_raw_data_export_safe(user_data)
logger.info(f"Processing: {safe_data}")

# ‚ùå Avoid - logging raw data
logger.info(f"Processing: {user_data}")  # might have secrets!
```

## Common Tasks

### Adding a New Function
1. Write the function with type hints
2. Add docstring (Google style)
3. Write tests (at least happy path + edge cases)
4. Update module `__all__` if public API
5. Run `ruff check` and `pytest`

### Fixing a Bug
1. Write a test that reproduces the bug
2. Fix the bug
3. Verify test passes
4. Check for similar bugs
5. Update documentation if needed

### Refactoring
1. Ensure tests exist and pass
2. Make changes incrementally
3. Run tests after each change
4. Verify type checking still passes
5. Update docstrings if behavior changed

## Error Messages

### Be Helpful
```python
# ‚úÖ Good - clear error with context
if not config_file.exists():
    raise FileNotFoundError(
        f"Config file not found: {config_file}. "
        f"Create it with: python setup.py init"
    )

# ‚ùå Avoid - vague error
if not config_file.exists():
    raise FileNotFoundError("Config not found")
```

## Documentation

### Docstring Format (Google Style)
```python
def process_items(items: list[dict], validate: bool = True) -> dict[str, Any]:
    """Process a list of items and return summary.
    
    Args:
        items: List of dictionaries containing item data
        validate: Whether to validate items before processing
        
    Returns:
        Dictionary with processing summary and statistics
        
    Raises:
        ValueError: If items list is empty or validation fails
        
    Example:
        >>> items = [{"id": 1, "name": "Item 1"}]
        >>> process_items(items)
        {"count": 1, "valid": 1}
    """
```

## Performance Tips

### Avoid Repeated Computation
```python
# ‚úÖ Good - compute once
unique_items = set(items)
for item in unique_items:
    process(item)

# ‚ùå Avoid - computing in loop
for item in items:
    if item not in processed:  # O(n) lookup each time
        process(item)
```

### Use Appropriate Data Structures
```python
# ‚úÖ Good - O(1) lookup
seen = set()
for item in items:
    if item not in seen:
        seen.add(item)

# ‚ùå Avoid - O(n) lookup
seen = []
for item in items:
    if item not in seen:  # Slow for large lists
        seen.append(item)
```

## Security

### Never Log Secrets
```python
# ‚úÖ Good - sanitize before logging
safe_config = {k: v for k, v in config.items() if k != "api_key"}
logger.info(f"Config: {safe_config}")

# ‚ùå Avoid - might log secrets
logger.info(f"Config: {config}")
```

### Validate Input
```python
# ‚úÖ Good - validate before use
def load_file(filepath: str) -> str:
    path = Path(filepath)
    if not path.is_file():
        raise ValueError(f"Not a file: {filepath}")
    if not path.suffix == ".json":
        raise ValueError(f"Not a JSON file: {filepath}")
    return path.read_text()
```

## Questions?

- Check `.ruler/AGENTS.md` for comprehensive guide
- Check `TEMPLATE_USAGE.md` for template setup
- Check `README.md` for project overview
- Don't suggest changes to CalVer/versioning approach

---

**Copilot Instructions Version:** 1.0
**Compatible With:** GitHub Copilot, Copilot Chat
**Last Updated:** 2025-11-25



<!-- Source: .ruler/cursor.md -->

# Cursor-Specific Agent Configuration

This file contains Cursor AI specific instructions not covered by standard ruler configuration.

## üîë CRITICAL: Authentication (READ FIRST!)

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
```

The default `GH_TOKEN` does NOT have jbcom access. NEVER use bare `gh` commands for jbcom repos.

## Background Agent Modes

### Code Review Mode
When reviewing code in PRs:
- Focus on logic and correctness
- Check type safety
- Verify test coverage
- Look for security issues
- Don't suggest CalVer/versioning changes

### Maintenance Mode
For routine maintenance tasks:
- Update dependencies
- Fix linting issues
- Improve documentation
- Refactor for clarity
- Keep it simple

### Migration Mode
When migrating repos to template:
- Follow TEMPLATE_USAGE.md exactly
- Update all placeholders
- Test locally before pushing
- Verify CI passes
- Don't skip steps

### Ecosystem Coordination Mode
When working across multiple repos:
- Start with extended-data-types
- Work in dependency order
- Test each repo independently
- Create PRs for each repo
- Don't merge until all PRs ready

## Custom Prompts

### /ecosystem-status
Check health of all ecosystem repositories:
- Clone or fetch all managed repos
- Check git status
- Review open issues/PRs
- Check CI status
- Report summary

### /update-dependencies
Update dependencies across ecosystem:
- Check for security updates
- Update pyproject.toml files
- Run tests in each repo
- Create PRs if tests pass

### /sync-template
Sync changes from template to managed repos:
- Identify changed files in template
- Apply to each managed repo
- Test each repo
- Create PRs

### /release-check
Pre-release verification:
- All tests passing
- No linting issues
- CHANGELOG.md updated
- Version will auto-increment
- Dependencies up to date

## Conversation Context

### Multi-file Context
When working on related files:
- Keep all related files in context
- Show diffs side-by-side
- Explain cross-file impacts
- Test changes together

### Long-running Tasks
For tasks requiring multiple steps:
- Break into subtasks
- Mark progress clearly
- Save state between steps
- Resume where left off
- Final verification step

## Error Handling

### CI Failures
When CI fails:
1. Read full error output
2. Identify root cause
3. Fix the issue
4. Re-run locally if possible
5. Push fix
6. Verify CI passes

### Type Check Errors
When mypy/pyright fails:
1. Read the specific error
2. Check if it's a real issue
3. Fix with proper type hints
4. Don't use `type: ignore` unless necessary
5. Document why if you must ignore

### Test Failures
When tests fail:
1. Read test output carefully
2. Identify which test failed
3. Understand what it's testing
4. Fix the code or the test
5. Run full test suite
6. Check coverage didn't drop

## Workflow Shortcuts

### Quick Fixes
For simple, obvious fixes:
- Make the change
- Run tests
- Push directly to PR branch
- No need to ask permission

### Breaking Changes
For changes that might break things:
- Explain the impact
- Show the changes
- Ask for confirmation
- Test thoroughly
- Monitor after merge

### Template Updates
When updating the template:
- Test in template repo first
- Verify all workflows pass
- Then update managed repos
- One repo at a time
- Verify each before next

## Code Style Preferences

### Python Style
- Use modern type hints (list[], dict[], not List[], Dict[])
- Prefer pathlib over os.path
- Use context managers for resources
- Keep functions focused and small
- Docstrings for public APIs

### Documentation Style
- Clear, concise language
- Examples for complex features
- Link to related docs
- Update when code changes
- Keep README up to date

### Test Style
- Descriptive test names
- One assertion per test (usually)
- Use fixtures for setup
- Test edge cases
- Mock external dependencies

## Performance Considerations

### Fast Operations
Prefer when possible:
- Parallel tool calls
- Batch operations
- Caching results
- Lazy loading
- Early returns

### Avoid
When possible avoid:
- Sequential operations that could be parallel
- Redundant file reads
- Unnecessary git operations
- Large output dumps
- Polling for status

## Communication Style

### With User
- Be concise
- Highlight important info
- Use formatting for clarity
- Show progress on long tasks
- Ask questions when unclear

### In Code Comments
- Explain why, not what
- Link to issues/PRs for context
- Update when code changes
- Remove outdated comments
- Keep them brief

### In Commit Messages
- Clear, descriptive
- Explain the change
- Reference issues if applicable
- No need for conventional commits
- But be informative

---

**Cursor Version:** Compatible with latest Cursor AI
**Last Updated:** 2025-11-25
**Maintained By:** python-library-template



<!-- Source: .ruler/ecosystem.md -->

# Ecosystem Repositories

This control center manages the jbcom Python library ecosystem via **MONOREPO ARCHITECTURE**.

---

## üèóÔ∏è ARCHITECTURE: All Code Lives Here

**ALL Python ecosystem code is in `packages/` in this repository.**

```
jbcom-control-center/packages/
‚îú‚îÄ‚îÄ extended-data-types/    ‚Üí syncs to ‚Üí jbcom/extended-data-types ‚Üí PyPI
‚îú‚îÄ‚îÄ lifecyclelogging/       ‚Üí syncs to ‚Üí jbcom/lifecyclelogging ‚Üí PyPI
‚îú‚îÄ‚îÄ directed-inputs-class/  ‚Üí syncs to ‚Üí jbcom/directed-inputs-class ‚Üí PyPI
‚îî‚îÄ‚îÄ vendor-connectors/      ‚Üí syncs to ‚Üí jbcom/vendor-connectors ‚Üí PyPI
```

### Workflow
1. **Edit** code in `packages/`
2. **PR** to control-center main
3. **Sync workflow** creates PRs in public repos
4. **Merge** public PRs ‚Üí CI ‚Üí PyPI release

### Why Monorepo
- ‚úÖ No cloning external repos
- ‚úÖ No GitHub API gymnastics
- ‚úÖ Single source of truth
- ‚úÖ Cross-package changes in ONE PR
- ‚úÖ Dependencies always aligned

---

## üì¶ Package Details

### 1. extended-data-types (FOUNDATION)
**Location:** `packages/extended-data-types/`
**PyPI:** `extended-data-types`
**Public Repo:** `jbcom/extended-data-types`

The foundation library - ALL other packages depend on this.

**Provides:**
- Re-exported libraries: `gitpython`, `inflection`, `lark`, `orjson`, `python-hcl2`, `ruamel.yaml`, `sortedcontainers`, `wrapt`
- Utilities: `strtobool`, `strtopath`, `make_raw_data_export_safe`, `get_unique_signature`
- Serialization: `decode_yaml`, `encode_yaml`, `decode_json`, `encode_json`
- Collections: `flatten_map`, `filter_map`, and more

**Rule:** Before adding ANY dependency to other packages, check if extended-data-types provides it.

### 2. lifecyclelogging
**Location:** `packages/lifecyclelogging/`
**PyPI:** `lifecyclelogging`
**Public Repo:** `jbcom/lifecyclelogging`

Structured lifecycle logging with automatic sanitization.

**Depends on:** extended-data-types

### 3. directed-inputs-class
**Location:** `packages/directed-inputs-class/`
**PyPI:** `directed-inputs-class`
**Public Repo:** `jbcom/directed-inputs-class`

Declarative input validation and processing.

**Depends on:** extended-data-types

### 4. vendor-connectors
**Location:** `packages/vendor-connectors/`
**PyPI:** `cloud-connectors`
**Public Repo:** `jbcom/vendor-connectors`

Unified cloud provider connectors (AWS, GCP, GitHub, Slack, Vault, Zoom).

**Depends on:** extended-data-types, lifecyclelogging

---

## üîó Dependency Chain

```
extended-data-types (FOUNDATION)
‚îú‚îÄ‚îÄ lifecyclelogging
‚îú‚îÄ‚îÄ directed-inputs-class
‚îî‚îÄ‚îÄ vendor-connectors (depends on BOTH)
```

**Release Order:** Always release in this order:
1. extended-data-types
2. lifecyclelogging  
3. directed-inputs-class
4. vendor-connectors

---

## üîß Working With Packages

### Edit Code
```bash
# Just edit files directly!
vim packages/extended-data-types/src/extended_data_types/type_utils.py
vim packages/vendor-connectors/pyproject.toml
```

### Run Tests
```bash
cd packages/extended-data-types && pip install -e ".[tests]" && pytest
cd packages/lifecyclelogging && pip install -e ".[tests]" && pytest
```

### Align Dependencies
```bash
# Update version across all packages
sed -i 's/extended-data-types>=.*/extended-data-types>=2025.11.200/' \
  packages/*/pyproject.toml
```

### Create PR
```bash
git checkout -b fix/whatever
git add -A && git commit -m "Fix: description"
git push -u origin fix/whatever
gh pr create --title "Fix: whatever"
```

---

## üîÑ Sync Configuration

### Files
- `packages/ECOSYSTEM.toml` - Source of truth
- `.github/sync.yml` - What syncs where
- `.github/workflows/sync-packages.yml` - Sync workflow

### Triggers
- Push to main with `packages/**` changes
- Manual workflow dispatch
- Release published

### Secret
`CI_GITHUB_TOKEN` from Doppler - has write access to all jbcom repos

---

## ‚ö†Ô∏è Rules

### DO
- ‚úÖ Edit code in `packages/` directly
- ‚úÖ Use regular git for this repo
- ‚úÖ Check `packages/ECOSYSTEM.toml` for relationships
- ‚úÖ Use extended-data-types utilities
- ‚úÖ Release in dependency order

### DON'T
- ‚ùå Clone external repos - code is HERE
- ‚ùå Add duplicate utilities
- ‚ùå Skip the sync workflow
- ‚ùå Push directly to main (use PRs)

---

## üéØ Eliminate Duplication

### Check Before Adding Dependencies
Always check `packages/extended-data-types/pyproject.toml` first.

### Red Flags
- `utils.py` > 100 lines ‚Üí duplicating extended-data-types
- Direct `import inflection` ‚Üí should use extended-data-types
- Custom JSON/YAML functions ‚Üí use `encode_json`, `decode_yaml`

### Correct Pattern
```python
# ‚úÖ Use foundation library
from extended_data_types import strtobool, make_raw_data_export_safe

# ‚ùå Don't reimplement
def my_str_to_bool(val):
    return val.lower() in ("true", "yes", "1")
```

---

## üìä Health Checks

### Check Public Repo CI
```bash
for repo in extended-data-types lifecyclelogging directed-inputs-class vendor-connectors; do
  gh run list --repo jbcom/$repo --limit 3
done
```

### Check PyPI Versions
```bash
pip index versions extended-data-types
pip index versions lifecyclelogging
pip index versions cloud-connectors
```

### Trigger Manual Sync
```bash
gh workflow run "Sync Packages to Public Repos" --repo jbcom/jbcom-control-center
```

---

**Source of Truth:** `packages/ECOSYSTEM.toml`
**All code is in:** `packages/`
**Sync handles:** Pushing to public repos and PyPI



<!-- Source: .ruler/README.md -->

# Ruler Directory - AI Agent Instructions

This directory contains the **single source of truth** for all AI agent instructions across the jbcom Python library ecosystem.

## How Ruler Works

Ruler is a framework that:
1. **Centralizes** all AI agent instructions in `.ruler/*.md` files
2. **Concatenates** these files in a specific order
3. **Distributes** the combined content to agent-specific configuration files

### File Processing Order

Ruler processes files in this order:
1. `AGENTS.md` (if present) - always first
2. Remaining `.md` files in sorted order

Current files:
1. **AGENTS.md** - Core guidelines (CalVer, PR workflow, common misconceptions)
2. **copilot.md** - Copilot-specific patterns and quick reference
3. **cursor.md** - Cursor agent modes, prompts, and workflows
4. **ecosystem.md** - Repository coordination and management

## Output Files

Ruler generates these files (DO NOT edit directly):

- **`.cursorrules`** - Cursor AI configuration
- **`.claud`** - Claude Code configuration  
- **`.github/copilot-instructions.md`** - GitHub Copilot instructions
- **`AGENTS.md`** (root) - For Aider and general AI agents

All these files have a "Generated by Ruler" header and source comments.

## Making Changes

### To Update Agent Instructions

1. **Edit files in `.ruler/` directory**
   ```bash
   vim .ruler/AGENTS.md        # Core guidelines
   vim .ruler/cursor.md        # Cursor-specific
   vim .ruler/copilot.md       # Copilot patterns
   vim .ruler/ecosystem.md     # Ecosystem coordination
   ```

2. **Apply ruler to regenerate**
   ```bash
   ruler apply
   ```

3. **Review changes**
   ```bash
   git diff .cursorrules .github/copilot-instructions.md AGENTS.md
   ```

4. **Commit everything**
   ```bash
   git add .ruler/ .cursorrules .github/copilot-instructions.md AGENTS.md
   git commit -m "Update agent instructions via ruler"
   ```

### Configuration

Edit `.ruler/ruler.toml` to configure:
- Which agents are active by default
- Custom output paths for specific agents
- Nested rule loading (for subdirectory-specific rules)

Current configuration:
```toml
default_agents = ["copilot", "cursor", "claude", "aider"]

[agents.copilot]
enabled = true
output_path = ".github/copilot-instructions.md"

[agents.cursor]
enabled = true
# Uses default .cursorrules

[agents.claude]
enabled = true
# Uses default .claud

[agents.aider]
enabled = true
output_path_instructions = "AGENTS.md"
```

## File Purposes

### AGENTS.md
**Primary audience:** All AI agents
**Content:**
- CalVer philosophy and rationale
- Why NOT to use semantic-release
- PR and release workflows
- Common agent misconceptions
- Development workflows
- Template maintenance guidelines

**Key sections:**
- CI/CD Design Philosophy
- Version Management
- Agent Approval Instructions
- Common Misconceptions

### copilot.md
**Primary audience:** GitHub Copilot
**Content:**
- Quick reference rules
- Code patterns and examples
- Testing patterns
- Common tasks (adding functions, fixing bugs, refactoring)
- Error message guidelines
- Security best practices

**Format:** Short, actionable examples with ‚úÖ/‚ùå comparisons

### cursor.md
**Primary audience:** Cursor AI
**Content:**
- Background agent modes (review, maintenance, migration, ecosystem)
- Custom prompts (`/ecosystem-status`, `/update-dependencies`, etc.)
- Error handling workflows
- Code style preferences
- Performance considerations
- Communication guidelines

**Format:** Operational guidelines for different work modes

### ecosystem.md
**Primary audience:** All agents doing cross-repo work
**Content:**
- Managed repository documentation
- Dependency graph
- Coordination guidelines
- Release coordination process
- Maintenance schedules
- Agent instructions for ecosystem work

**Format:** Reference documentation with procedures

## Best Practices

### Writing Agent Instructions

1. **Be explicit** - Don't assume agents understand context
2. **Use examples** - Show both good (‚úÖ) and bad (‚ùå) patterns
3. **Explain why** - Not just what to do, but why it matters
4. **Anticipate mistakes** - Document common misconceptions
5. **Keep updated** - Revise based on agent behavior

### Organizing Content

- **General guidelines** ‚Üí `AGENTS.md`
- **Agent-specific patterns** ‚Üí `{agent}.md`
- **Domain-specific** ‚Üí Separate files (e.g., `ecosystem.md`)
- **Quick reference** ‚Üí Use bullet points and code examples
- **Deep explanations** ‚Üí Use sections with rationale

### Testing Changes

After updating ruler content:

1. **Apply ruler**
   ```bash
   ruler apply
   ```

2. **Test with actual agents**
   - Ask Cursor to perform a task
   - Check if Copilot suggestions align
   - Verify agents follow new guidelines

3. **Iterate based on behavior**
   - If agents still make mistakes, clarify instructions
   - Add more examples if needed
   - Update misconceptions section

## Advanced Usage

### Dry Run

Preview what ruler will do:
```bash
ruler apply --dry-run
```

### Specific Agents

Apply only for certain agents:
```bash
ruler apply --agents copilot,cursor
```

### Verify Generated Files

After applying ruler, check that generated files:
1. Have "Generated by Ruler" header
2. Include all source files
3. Maintain proper formatting
4. Match expected structure

## Maintenance

### When to Update

Update ruler content when:
- Agents consistently misunderstand something
- New workflows or patterns emerge
- Ecosystem grows (new repos)
- Template changes significantly
- Agent tools/capabilities change

### Version History

Track changes to ruler content in git:
- Commit messages should explain what behavior changed
- Tag major instruction updates
- Document breaking changes for agents

### Testing Framework

Consider adding:
- Example prompts that test agent understanding
- Expected vs actual behavior documentation
- Agent response validation

## Integration with Template

This ruler setup is part of the python-library-template:

- **Template repos** - Use this exact structure
- **Managed repos** - Receive updates from template
- **Synchronization** - Ruler changes propagate via template updates

## Resources

- **Ruler Documentation:** https://github.com/intellectronica/ruler
- **Template Usage:** /workspace/TEMPLATE_USAGE.md
- **Ecosystem Guide:** /workspace/ECOSYSTEM.md

---

**Ruler Version:** Compatible with @intellectronica/ruler latest
**Last Updated:** 2025-11-25
**Maintained By:** python-library-template
