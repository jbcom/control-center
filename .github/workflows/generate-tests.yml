name: Generate Tests with AI

on:
  workflow_dispatch:
    inputs:
      repository:
        description: "Target Python repository"
        required: true
        type: choice
        options:
          - jbcom/agentic-crew
          - jbcom/ai_game_dev
          - jbcom/directed-inputs-class
          - jbcom/extended-data-types
          - jbcom/lifecyclelogging
          - jbcom/python-terraform-bridge
          - jbcom/rivers-of-reckoning
          - jbcom/vendor-connectors
        default: jbcom/extended-data-types
      model:
        description: "Ollama Cloud model to use"
        required: true
        type: choice
        options:
          # GLM-4.6:cloud - Current generation (from ollama.com)
          - glm-4.6:cloud       # Advanced reasoning and coding model
          - qwen3-coder:480b-cloud  # Specialized coding model
          - gpt-oss:120b-cloud  # Large general-purpose model
        default: glm-4.6:cloud
      base_branch:
        description: "Base branch to analyze"
        required: false
        type: string
        default: main
      coverage_threshold:
        description: "Target coverage % (enforced standard, overrides pyproject.toml)"
        required: false
        type: number
        default: 100
      use_pyproject_coverage:
        description: "Use coverage from pyproject.toml instead of enforcing workflow standard"
        required: false
        type: boolean
        default: false
      max_parallel_jobs:
        description: "Maximum parallel test generation jobs"
        required: false
        type: number
        default: 4
      dry_run:
        description: "Dry run (analyze only, don't generate)"
        required: false
        type: boolean
        default: false
      auto_fix_tests:
        description: "Auto-fix failing generated tests with Claude"
        required: false
        type: boolean
        default: true

permissions:
  contents: write
  pull-requests: write
  actions: read
  issues: write
  id-token: write

env:
  PYTHON_VERSION: "3.12"

jobs:
  # ============================================
  # ANALYZE - Discover modules needing tests
  # ============================================
  analyze:
    name: Analyze Coverage Gaps
    runs-on: ubuntu-latest
    outputs:
      modules_json: ${{ steps.discover.outputs.modules_json }}
      module_count: ${{ steps.discover.outputs.module_count }}
      has_modules: ${{ steps.discover.outputs.has_modules }}
      repo_name: ${{ steps.setup.outputs.repo_name }}
      effective_threshold: ${{ steps.threshold.outputs.effective_threshold }}
      pyproject_threshold: ${{ steps.threshold.outputs.pyproject_threshold }}

    steps:
      - name: Extract repo name
        id: setup
        run: |
          REPO="${{ inputs.repository }}"
          REPO_NAME="${REPO#*/}"
          echo "repo_name=$REPO_NAME" >> $GITHUB_OUTPUT
          echo "ðŸ“¦ Repository: $REPO"
          echo "ðŸ“› Repo name: $REPO_NAME"

      # actions/checkout v6.0.1
      - name: Checkout target repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8
        with:
          repository: ${{ inputs.repository }}
          ref: ${{ inputs.base_branch }}
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          fetch-depth: 0
          path: target-repo

      # actions/setup-python v6.1.0
      - name: Set up Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Determine coverage threshold
        id: threshold
        working-directory: target-repo
        run: |
          # Default to workflow input (100% by default)
          WORKFLOW_THRESHOLD="${{ inputs.coverage_threshold }}"
          PYPROJECT_THRESHOLD=""
          EFFECTIVE_THRESHOLD="$WORKFLOW_THRESHOLD"
          
          # Try to read from pyproject.toml
          if [ -f "pyproject.toml" ]; then
            # Extract fail_under from [tool.coverage.report] or [tool.pytest.ini_options]
            PYPROJECT_THRESHOLD=$(python3 -c "
          import sys
          try:
              import tomllib
          except ImportError:
              try:
                  import tomli as tomllib
              except ImportError:
                  print('')
                  sys.exit(0)
          
          with open('pyproject.toml', 'rb') as f:
              data = tomllib.load(f)
          
          # Check tool.coverage.report.fail_under
          fail_under = data.get('tool', {}).get('coverage', {}).get('report', {}).get('fail_under')
          if fail_under is not None:
              print(int(fail_under))
              sys.exit(0)
          
          # Check tool.pytest.ini_options for cov-fail-under
          pytest_opts = data.get('tool', {}).get('pytest', {}).get('ini_options', {})
          addopts = pytest_opts.get('addopts', '')
          if '--cov-fail-under' in addopts:
              import re
              match = re.search(r'--cov-fail-under[=\s]+(\d+)', addopts)
              if match:
                  print(match.group(1))
                  sys.exit(0)
          
          print('')
          " 2>/dev/null || echo "")
          fi
          
          echo "pyproject_threshold=$PYPROJECT_THRESHOLD" >> $GITHUB_OUTPUT
          
          # Use pyproject threshold if checkbox is checked AND pyproject has a value
          if [ "${{ inputs.use_pyproject_coverage }}" == "true" ] && [ -n "$PYPROJECT_THRESHOLD" ]; then
            EFFECTIVE_THRESHOLD="$PYPROJECT_THRESHOLD"
            echo "ðŸ“‹ Using pyproject.toml coverage threshold: ${EFFECTIVE_THRESHOLD}%"
          else
            echo "ðŸŽ¯ Enforcing workflow coverage standard: ${EFFECTIVE_THRESHOLD}%"
            if [ -n "$PYPROJECT_THRESHOLD" ]; then
              echo "   (pyproject.toml has: ${PYPROJECT_THRESHOLD}%)"
            fi
          fi
          
          echo "effective_threshold=$EFFECTIVE_THRESHOLD" >> $GITHUB_OUTPUT

      - name: Install dependencies
        working-directory: target-repo
        run: |
          python -m pip install --upgrade pip
          pip install coverage pytest pytest-cov toml pyyaml tomli

          # Install repo dependencies if available
          if [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]" 2>/dev/null || pip install -e . 2>/dev/null || true
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt 2>/dev/null || true
          fi
          if [ -f "requirements-dev.txt" ]; then
            pip install -r requirements-dev.txt 2>/dev/null || true
          fi

      - name: Run coverage analysis
        id: coverage
        working-directory: target-repo
        continue-on-error: true
        run: |
          # Try to run existing tests with coverage
          if [ -d "tests" ] || [ -d "test" ]; then
            pytest --cov=. --cov-report=json --cov-report=term-missing -q 2>/dev/null || true
          fi

          # If coverage.json exists, we have coverage data
          if [ -f "coverage.json" ]; then
            echo "has_coverage=true" >> $GITHUB_OUTPUT
          else
            echo "has_coverage=false" >> $GITHUB_OUTPUT
          fi

      - name: Discover modules needing tests
        id: discover
        working-directory: target-repo
        run: |
          cat > discover_modules.py << 'PYTHON_SCRIPT'
          import os
          import json
          import sys
          from pathlib import Path
          import ast

          def get_package_dirs():
              """Find Python package directories."""
              packages = []
              
              # Check for src layout
              if Path("src").is_dir():
                  for item in Path("src").iterdir():
                      if item.is_dir() and (item / "__init__.py").exists():
                          packages.append(str(item))
              
              # Check for flat layout
              for item in Path(".").iterdir():
                  if item.is_dir() and item.name not in ("tests", "test", "docs", "build", "dist", ".git", ".venv", "venv", "__pycache__", ".tox", "htmlcov", "src"):
                      if (item / "__init__.py").exists():
                          packages.append(str(item))
              
              return packages or ["."]

          def find_python_modules(package_dirs):
              """Find all Python modules in package directories."""
              modules = []
              
              for pkg_dir in package_dirs:
                  pkg_path = Path(pkg_dir)
                  for py_file in pkg_path.rglob("*.py"):
                      # Skip test files, __pycache__, etc
                      rel_path = str(py_file)
                      if any(x in rel_path for x in ["__pycache__", "test_", "_test.py", "tests/", "test/", "conftest.py"]):
                          continue
                      if py_file.name.startswith("_") and py_file.name != "__init__.py":
                          continue
                      
                      # Get module info
                      try:
                          with open(py_file, "r", encoding="utf-8") as f:
                              content = f.read()
                          tree = ast.parse(content)
                          
                          # Count functions, classes, methods
                          funcs = sum(1 for n in ast.walk(tree) if isinstance(n, ast.FunctionDef))
                          classes = sum(1 for n in ast.walk(tree) if isinstance(n, ast.ClassDef))
                          
                          if funcs > 0 or classes > 0:  # Only include files with code
                              modules.append({
                                  "path": str(py_file),
                                  "functions": funcs,
                                  "classes": classes,
                                  "priority": classes * 2 + funcs  # Weight classes more
                              })
                      except (SyntaxError, UnicodeDecodeError, OSError):
                          continue
              
              return sorted(modules, key=lambda x: x["priority"], reverse=True)

          def check_existing_tests(modules):
              """Check which modules already have tests."""
              test_dirs = ["tests", "test"]
              result = []
              
              for module in modules:
                  module_path = Path(module["path"])
                  module_name = module_path.stem
                  
                  # Look for existing test file
                  has_test = False
                  for test_dir in test_dirs:
                      if Path(test_dir).is_dir():
                          test_patterns = [
                              f"test_{module_name}.py",
                              f"{module_name}_test.py",
                              f"test_{module_path.parent.name}_{module_name}.py"
                          ]
                          for pattern in test_patterns:
                              if list(Path(test_dir).rglob(pattern)):
                                  has_test = True
                                  break
                  
                  module["has_test"] = has_test
                  if not has_test:
                      result.append(module)
              
              return result

          def apply_coverage_filter(modules, threshold):
              """Filter modules based on coverage data if available."""
              if not Path("coverage.json").exists():
                  return modules
              
              try:
                  with open("coverage.json", "r") as f:
                      cov_data = json.load(f)
                  
                  files_cov = cov_data.get("files", {})
                  filtered = []
                  
                  for module in modules:
                      file_cov = files_cov.get(module["path"], {})
                      summary = file_cov.get("summary", {})
                      covered_pct = summary.get("percent_covered", 0)
                      
                      module["coverage"] = covered_pct
                      if covered_pct < threshold:
                          filtered.append(module)
                  
                  return filtered
              except (json.JSONDecodeError, FileNotFoundError, KeyError):
                  return modules

          def group_by_directory(modules, max_per_group=3):
              """Group modules by directory for parallel processing."""
              from collections import defaultdict
              
              groups = defaultdict(list)
              for module in modules:
                  dir_path = str(Path(module["path"]).parent)
                  groups[dir_path].append(module)
              
              # Create job definitions
              jobs = []
              for dir_path, dir_modules in groups.items():
                  # Split large directories into multiple jobs
                  for i in range(0, len(dir_modules), max_per_group):
                      batch = dir_modules[i:i+max_per_group]
                      jobs.append({
                          "directory": dir_path,
                          "modules": [m["path"] for m in batch],
                          "priority": sum(m["priority"] for m in batch),
                          "job_id": f"{dir_path.replace('/', '_').replace('.', '_')}_{i//max_per_group}"
                      })
              
              return sorted(jobs, key=lambda x: x["priority"], reverse=True)

          # Main execution
          threshold = int(os.environ.get("COVERAGE_THRESHOLD", "100"))
          max_jobs = int(os.environ.get("MAX_PARALLEL_JOBS", "4"))

          packages = get_package_dirs()
          print(f"ðŸ“¦ Found packages: {packages}", file=sys.stderr)

          modules = find_python_modules(packages)
          print(f"ðŸ“„ Found {len(modules)} Python modules", file=sys.stderr)

          modules = check_existing_tests(modules)
          print(f"ðŸ” {len(modules)} modules without tests", file=sys.stderr)

          modules = apply_coverage_filter(modules, threshold)
          print(f"ðŸ“Š {len(modules)} modules below {threshold}% coverage threshold", file=sys.stderr)

          jobs = group_by_directory(modules)[:max_jobs]
          print(f"âš¡ Created {len(jobs)} parallel job groups", file=sys.stderr)

          # Output for GitHub Actions
          output = {
              "modules": jobs,
              "total_files": len(modules),
              "job_count": len(jobs)
          }

          print(json.dumps(output))
          PYTHON_SCRIPT

          RESULT=$(python discover_modules.py)
          echo "Discovery result: $RESULT"
          
          MODULES_JSON=$(echo "$RESULT" | jq -c '.modules')
          MODULE_COUNT=$(echo "$RESULT" | jq '.job_count')
          
          echo "modules_json=$MODULES_JSON" >> $GITHUB_OUTPUT
          echo "module_count=$MODULE_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$MODULE_COUNT" -gt 0 ]; then
            echo "has_modules=true" >> $GITHUB_OUTPUT
            echo "âœ… Found $MODULE_COUNT module groups to generate tests for"
          else
            echo "has_modules=false" >> $GITHUB_OUTPUT
            echo "âœ… All modules have adequate test coverage!"
          fi
        env:
          COVERAGE_THRESHOLD: ${{ steps.threshold.outputs.effective_threshold }}
          MAX_PARALLEL_JOBS: ${{ inputs.max_parallel_jobs }}

      - name: Display analysis summary
        run: |
          echo "## ðŸ“Š Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Repository | \`${{ inputs.repository }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Branch | \`${{ inputs.base_branch }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| **Effective Coverage Threshold** | **${{ steps.threshold.outputs.effective_threshold }}%** |" >> $GITHUB_STEP_SUMMARY
          echo "| Workflow Standard | ${{ inputs.coverage_threshold }}% |" >> $GITHUB_STEP_SUMMARY
          PYPROJECT_VAL="${{ steps.threshold.outputs.pyproject_threshold }}"
          if [ -n "$PYPROJECT_VAL" ]; then
            echo "| pyproject.toml Value | ${PYPROJECT_VAL}% |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| pyproject.toml Value | N/A |" >> $GITHUB_STEP_SUMMARY
          fi
          echo "| Using pyproject.toml? | ${{ inputs.use_pyproject_coverage }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Modules Found | ${{ steps.discover.outputs.module_count }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Model | \`${{ inputs.model }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.discover.outputs.has_modules }}" == "true" ]; then
            echo "### ðŸ“¦ Module Groups to Process" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            echo '${{ steps.discover.outputs.modules_json }}' | jq '.' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "âœ… **All modules have adequate test coverage!**" >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================
  # GENERATE - Create tests in parallel
  # ============================================
  generate:
    name: Generate Tests
    needs: analyze
    if: needs.analyze.outputs.has_modules == 'true' && inputs.dry_run != true
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      max-parallel: ${{ inputs.max_parallel_jobs }}
      matrix:
        module: ${{ fromJSON(needs.analyze.outputs.modules_json) }}

    steps:
      # actions/checkout v6.0.1
      - name: Checkout target repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8
        with:
          repository: ${{ inputs.repository }}
          ref: ${{ inputs.base_branch }}
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          fetch-depth: 0

      # actions/setup-python v6.1.0
      - name: Set up Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov

          # Install repo dependencies
          if [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]" 2>/dev/null || pip install -e . 2>/dev/null || true
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt 2>/dev/null || true
          fi

      # Install Ollama CLI
      - name: Install Ollama CLI
        run: curl -fsSL https://ollama.com/install.sh | sh

      # Configure for Cloud
      - name: Configure Ollama for Cloud
        run: |
          echo "OLLAMA_HOST=https://ollama.com" >> $GITHUB_ENV
          echo "OLLAMA_API_KEY=${{ secrets.OLLAMA_API_KEY }}" >> $GITHUB_ENV

      # Register cloud model
      - name: Register cloud model
        run: ollama pull ${{ inputs.model }}

      # Generate tests with Ollama
      - name: Generate tests with Ollama
        id: generate
        run: |
          # Get the source files
          MODULES="${{ toJSON(matrix.module.modules) }}"
          
          # Build the prompt
          PROMPT="You are a Python test generation expert. Generate comprehensive unit tests for the following modules.

          ## Target Modules
          $MODULES

          ## Instructions
          1. Read each source file carefully to understand its functionality
          2. Generate pytest-compatible tests with:
             - Given-When-Then structure in docstrings
             - Comprehensive edge case coverage
             - Proper mocking of external dependencies
             - Parameterized tests where appropriate
             - Clear, descriptive test names following \`test_<function>_<scenario>\` pattern

          3. Create test files in the \`tests/\` directory:
             - For \`src/pkg/module.py\` â†’ \`tests/test_module.py\` or \`tests/pkg/test_module.py\`
             - For \`pkg/module.py\` â†’ \`tests/test_module.py\` or \`tests/pkg/test_module.py\`

          4. Include:
             - All necessary imports
             - Fixtures for common setup
             - pytest.mark decorators for slow/integration tests
             - Docstrings explaining test purpose

          5. Ensure tests are isolated and don't depend on external state

          6. Target at least ${{ needs.analyze.outputs.effective_threshold }}% code coverage for each module

          ## Output
          Create the test files directly. Do not output explanations, only code.
          After creating tests, run \`pytest <test_file> -v\` to verify they pass.
          Fix any failing tests before committing."

          # Send to Ollama API
          RESPONSE=$(curl -s --fail-with-body --max-time 300  \
            -H "Authorization: Bearer ${{ secrets.OLLAMA_API_KEY }}" \
            -d "{
              \"model\": \"${{ inputs.model }}\",
              \"messages\": [
                {
                  \"role\": \"system\",
                  \"content\": \"You are an expert Python test generation assistant.\"
                },
                {
                  \"role\": \"user\",
                  \"content\": \"$PROMPT\"
                }
              ],
              \"options\": {
                \"temperature\": 0.1,
                \"mirostat\": 2,
                \"mirostat_tau\": 5.0,
                \"mirostat_eta\": 0.1,
                \"top_p\": 0.95,
                \"repeat_penalty\": 1.15,
                \"num_ctx\": 196608,
                \"num_predict\": 4096
              },
              \"stream\": false
            }")

          if [ $? -ne 0 ]; then
            echo "Error: Failed to connect to Ollama API"
            exit 1
          fi

          # Extract the response content
          TEST_CODE=$(echo "$RESPONSE" | jq -r '.message.content // empty')

          if [ -z "$TEST_CODE" ] || [ "$TEST_CODE" = "null" ]; then
            echo "Error: Empty or invalid response from model"
            exit 1
          fi

          echo "Generated test code successfully"
        env:
          GH_TOKEN: ${{ github.token }}

      # tj-actions/changed-files v47.0.1
      - name: Get changed files
        id: changed
        uses: tj-actions/changed-files@e0021407031f5be11a464abee9a0776171c79891
        with:
          files: |
            tests/**/*.py
            test/**/*.py

      # actions/upload-artifact v6.0.0
      - name: Upload generated tests
        if: steps.changed.outputs.any_changed == 'true'
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f
        with:
          name: generated-tests-${{ matrix.module.job_id }}
          path: |
            tests/
            test/
          retention-days: 1

  # ============================================
  # CONSOLIDATE - Merge all generated tests
  # ============================================
  consolidate:
    name: Consolidate & Create PR
    needs: [analyze, generate]
    if: always() && needs.analyze.outputs.has_modules == 'true' && inputs.dry_run != true && needs.generate.result != 'cancelled'
    runs-on: ubuntu-latest
    outputs:
      pr_number: ${{ steps.create-pr.outputs.pull-request-number }}
      pr_url: ${{ steps.create-pr.outputs.pull-request-url }}
      tests_pass: ${{ steps.verify.outputs.tests_pass }}

    steps:
      # actions/checkout v6.0.1
      - name: Checkout target repository
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8
        with:
          repository: ${{ inputs.repository }}
          ref: ${{ inputs.base_branch }}
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          fetch-depth: 0

      # actions/download-artifact v7.0.0
      - name: Download all generated tests
        uses: actions/download-artifact@37930b1c2abaa49bbe596cd826c3c89aef350131
        with:
          pattern: generated-tests-*
          merge-multiple: true

      # actions/setup-python v6.1.0
      - name: Set up Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies and verify tests
        id: verify
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov

          # Install repo dependencies
          if [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]" 2>/dev/null || pip install -e . 2>/dev/null || true
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt 2>/dev/null || true
          fi

          # Run tests to verify they work
          echo "ðŸ§ª Running generated tests..."
          set -o pipefail
          if pytest tests/ -v --tb=short 2>&1 | tee test_output.txt; then
            echo "tests_pass=true" >> $GITHUB_OUTPUT
            echo "âœ… All generated tests pass!"
          else
            echo "tests_pass=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ Some tests failed - will attempt auto-fix"
          fi

      - name: Check for generated test files
        id: check-tests
        run: |
          # Check if tests directory has any Python files (downloaded artifacts are untracked)
          if find tests/ -name "*.py" -type f 2>/dev/null | grep -q .; then
            echo "has_tests=true" >> $GITHUB_OUTPUT
            echo "âœ… Found test files in tests/"
          elif find test/ -name "*.py" -type f 2>/dev/null | grep -q .; then
            echo "has_tests=true" >> $GITHUB_OUTPUT
            echo "âœ… Found test files in test/"
          else
            echo "has_tests=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ No test files found"
          fi

      # peter-evans/create-pull-request v8.0.0
      - name: Create Pull Request
        if: steps.check-tests.outputs.has_tests == 'true'
        id: create-pr
        uses: peter-evans/create-pull-request@98357b18bf14b5342f975ff684046ec3b2a07725
        with:
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          commit-message: |
            test: add AI-generated unit tests

            Generated by Claude (${{ inputs.model }}) via generate-tests workflow.
            Coverage threshold: ${{ needs.analyze.outputs.effective_threshold }}%

            Modules tested:
            ${{ needs.analyze.outputs.modules_json }}
          branch: ai-generated-tests/${{ github.run_id }}
          delete-branch: true
          title: "ðŸ§ª AI-Generated Unit Tests"
          body: |
            ## ðŸ¤– AI-Generated Tests

            This PR contains automatically generated unit tests created by Claude (`${{ inputs.model }}`).

            ### ðŸ“Š Generation Details
            | Metric | Value |
            |--------|-------|
            | Repository | `${{ inputs.repository }}` |
            | Model | `${{ inputs.model }}` |
            | Coverage Threshold | ${{ needs.analyze.outputs.effective_threshold }}% |
            | Module Groups | ${{ needs.analyze.outputs.module_count }} |
            | Tests Passing | ${{ steps.verify.outputs.tests_pass == 'true' && 'âœ… Yes' || 'âš ï¸ Needs fixes' }} |

            ### ðŸ“ Test Files
            Run `git diff --name-only` on the PR branch to see all generated files.

            ### âœ… Review Checklist
            - [ ] Tests follow project conventions
            - [ ] Edge cases are properly covered
            - [ ] Mocking is appropriate
            - [ ] No sensitive data in tests
            - [ ] All tests pass locally

            ---
            *Generated by [generate-tests.yml](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})*
          labels: |
            tests
            ai-generated
            automated

      - name: Output PR info
        if: steps.create-pr.outputs.pull-request-number
        run: |
          echo "## ðŸŽ‰ Pull Request Created!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**PR:** #${{ steps.create-pr.outputs.pull-request-number }}" >> $GITHUB_STEP_SUMMARY
          echo "**URL:** ${{ steps.create-pr.outputs.pull-request-url }}" >> $GITHUB_STEP_SUMMARY

  # ============================================
  # AUTO-FIX - Fix failing tests with Claude
  # ============================================
  auto-fix:
    name: Auto-Fix Failing Tests
    needs: [analyze, consolidate]
    if: |
      always() &&
      needs.consolidate.outputs.pr_number &&
      needs.consolidate.outputs.tests_pass != 'true' &&
      inputs.auto_fix_tests != false
    runs-on: ubuntu-latest
    outputs:
      tests_pass: ${{ steps.run-tests.outputs.all_pass }}

    steps:
      # actions/checkout v6.0.1
      - name: Checkout PR branch
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8
        with:
          repository: ${{ inputs.repository }}
          ref: ai-generated-tests/${{ github.run_id }}
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          fetch-depth: 0

      # actions/setup-python v6.1.0
      - name: Set up Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov

          if [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]" 2>/dev/null || pip install -e . 2>/dev/null || true
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt 2>/dev/null || true
          fi

      - name: Run tests and capture failures
        id: run-tests
        continue-on-error: true
        run: |
          pytest tests/ -v --tb=long 2>&1 | tee test_results.txt
          
          if [ ${PIPESTATUS[0]} -eq 0 ]; then
            echo "all_pass=true" >> $GITHUB_OUTPUT
          else
            echo "all_pass=false" >> $GITHUB_OUTPUT
          fi

      # anthropics/claude-code-action v1
      # Install Ollama CLI
      - name: Install Ollama CLI
        run: curl -fsSL https://ollama.com/install.sh | sh

      # Configure for Cloud
      - name: Configure Ollama for Cloud
        run: |
          echo "OLLAMA_HOST=https://ollama.com" >> $GITHUB_ENV
          echo "OLLAMA_API_KEY=${{ secrets.OLLAMA_API_KEY }}" >> $GITHUB_ENV

      # Register cloud model
      - name: Register cloud model
        run: ollama pull ${{ inputs.model }}

      # Fix failing tests with Ollama
      - name: Fix failing tests with Ollama
        if: steps.run-tests.outputs.all_pass == 'false'
        run: |
          # Read test results
          TEST_RESULTS=$(cat test_results.txt)
          
          # Build the prompt
          PROMPT="You are a Python test debugging expert. Fix the failing tests in this repository.

          ## Test Results
          $TEST_RESULTS

          ## Instructions
          1. Read test_results.txt to understand what failed
          2. For each failing test:
             - Understand the assertion error or exception
             - Check if the test logic is wrong or the source code behavior changed
             - Fix the test to correctly validate the expected behavior
          3. Common fixes needed:
             - Incorrect mock setup
             - Wrong expected values
             - Missing imports
             - Incorrect test isolation
             - Async test issues
          4. Re-run pytest after each fix to verify
          5. Continue until all tests pass

          ## Output
          Fix the test files directly. Run pytest to verify fixes work.
          Do not change source code - only fix the tests."

          # Send to Ollama API
          RESPONSE=$(curl -s https://ollama.com/api/chat \
            -H "Authorization: Bearer ${{ secrets.OLLAMA_API_KEY }}" \
            -d "{
              \"model\": \"${{ inputs.model }}\",
              \"messages\": [
                {
                  \"role\": \"system\",
                  \"content\": \"You are an expert Python test debugging assistant.\"
                },
                {
                  \"role\": \"user\",
                  \"content\": \"$PROMPT\"
                }
              ],
              \"options\": {
                \"temperature\": 0.1,
                \"mirostat\": 2,
                \"mirostat_tau\": 5.0,
                \"mirostat_eta\": 0.1,
                \"top_p\": 0.95,
                \"repeat_penalty\": 1.15,
                \"num_ctx\": 196608,
                \"num_predict\": 4096
              },
              \"stream\": false
            }")

          # Extract the response content
          FIX_CODE=$(echo "$RESPONSE" | jq -r '.message.content // empty')

          if [ -z "$FIX_CODE" ] || [ "$FIX_CODE" = "null" ]; then
            echo "Error: Empty or invalid response from model"
            exit 1
          fi

          echo "Applied test fixes successfully"
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Verify fixes and push
        id: verify-push
        run: |
          # Run tests again
          if pytest tests/ -v; then
            echo "âœ… All tests now pass!"
            echo "final_pass=true" >> $GITHUB_OUTPUT
            
            # Commit and push fixes
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            
            if git diff --quiet; then
              echo "No changes to commit"
            else
              git add -A
              git commit -m "fix(tests): auto-fix failing generated tests

              Fixed by Claude (${{ inputs.model }}) via auto-fix workflow."
              git push
              echo "âœ… Pushed test fixes"
            fi
          else
            echo "âš ï¸ Some tests still failing - manual intervention needed"
            echo "final_pass=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Comment on PR with fix status
        if: always()
        env:
          GH_TOKEN: ${{ secrets.CI_GITHUB_TOKEN }}
        run: |
          PR_NUM="${{ needs.consolidate.outputs.pr_number }}"
          REPO="${{ inputs.repository }}"
          
          if [ "${{ steps.run-tests.outputs.all_pass }}" == "true" ]; then
            BODY="âœ… **All generated tests pass!** Ready for review."
          else
            BODY="ðŸ”§ **Auto-fix attempted** for failing tests. Please review the latest changes."
          fi
          
          gh pr comment "$PR_NUM" --repo "$REPO" --body "$BODY"

  # ============================================
  # UPDATE-PYPROJECT - Update coverage standard
  # ============================================
  update-pyproject:
    name: Update pyproject.toml Coverage Standard
    needs: [analyze, consolidate, auto-fix]
    if: |
      always() &&
      needs.consolidate.outputs.pr_number &&
      (needs.auto-fix.result == 'success' || needs.auto-fix.result == 'skipped') &&
      inputs.use_pyproject_coverage != true
    runs-on: ubuntu-latest

    steps:
      # actions/checkout v6.0.1
      - name: Checkout PR branch
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8
        with:
          repository: ${{ inputs.repository }}
          ref: ai-generated-tests/${{ github.run_id }}
          token: ${{ secrets.CI_GITHUB_TOKEN }}
          fetch-depth: 0

      # actions/setup-python v6.1.0
      - name: Set up Python
        uses: actions/setup-python@83679a892e2d95755f2dac6acb0bfd1e9ac5d548
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov tomli tomli-w

          if [ -f "pyproject.toml" ]; then
            pip install -e ".[dev,test]" 2>/dev/null || pip install -e . 2>/dev/null || true
          elif [ -f "requirements.txt" ]; then
            pip install -r requirements.txt 2>/dev/null || true
          fi

      - name: Measure actual coverage
        id: measure
        run: |
          # Run tests with coverage
          pytest tests/ --cov=. --cov-report=json --cov-report=term -q 2>&1 | tee coverage_output.txt || true
          
          # Extract total coverage
          if [ -f "coverage.json" ]; then
            ACTUAL_COV=$(python3 -c "import json; print(int(json.load(open('coverage.json'))['totals']['percent_covered']))")
            echo "actual_coverage=$ACTUAL_COV" >> $GITHUB_OUTPUT
            echo "ðŸ“Š Actual coverage achieved: ${ACTUAL_COV}%"
          else
            echo "actual_coverage=0" >> $GITHUB_OUTPUT
            echo "âš ï¸ Could not measure coverage"
          fi

      # Install Ollama CLI
      - name: Install Ollama CLI
        run: curl -fsSL https://ollama.com/install.sh | sh

      # Configure for Cloud
      - name: Configure Ollama for Cloud
        run: |
          echo "OLLAMA_HOST=https://ollama.com" >> $GITHUB_ENV
          echo "OLLAMA_API_KEY=${{ secrets.OLLAMA_API_KEY }}" >> $GITHUB_ENV

      # Register cloud model
      - name: Register cloud model
        run: ollama pull ${{ inputs.model }}

      # Update pyproject.toml with Ollama
      - name: Update pyproject.toml with Ollama
        if: steps.measure.outputs.actual_coverage != '0'
        run: |
          # Read pyproject.toml
          PYPROJECT_CONTENT=$(cat pyproject.toml)
          
          # Build the prompt
          PROMPT="You are updating the pyproject.toml to enforce a new minimum code coverage standard.

          ## Context
          - Target coverage threshold from workflow: ${{ needs.analyze.outputs.effective_threshold }}%
          - Actual coverage achieved: ${{ steps.measure.outputs.actual_coverage }}%
          - Previous pyproject.toml threshold: ${{ needs.analyze.outputs.pyproject_threshold || 'not set' }}%

          ## Task
          Update pyproject.toml to set the coverage fail_under value to the ACHIEVED coverage (${{ steps.measure.outputs.actual_coverage }}%).

          This ensures the new coverage standard is enforced going forward - coverage can only go UP, never down.

          ## Instructions
          1. Read pyproject.toml
          2. Find or create [tool.coverage.report] section
          3. Set fail_under = ${{ steps.measure.outputs.actual_coverage }}
          4. If the file uses pytest addopts with --cov-fail-under, update that instead
          5. Preserve all other settings
          6. Write the updated file

          Example of what to add/update:
          \`\`\`toml
          [tool.coverage.report]
          fail_under = ${{ steps.measure.outputs.actual_coverage }}
          \`\`\`

          ## Important
          - Only update pyproject.toml, no other files
          - Do NOT lower the threshold if it's already higher
          - Preserve formatting and comments where possible

          Current pyproject.toml content:
          \`\`\`
          $PYPROJECT_CONTENT
          \`\`\`"

          # Send to Ollama API
          RESPONSE=$(curl -s https://ollama.com/api/chat \
            -H "Authorization: Bearer ${{ secrets.OLLAMA_API_KEY }}" \
            -d "{
              \"model\": \"${{ inputs.model }}\",
              \"messages\": [
                {
                  \"role\": \"system\",
                  \"content\": \"You are an expert Python configuration assistant.\"
                },
                {
                  \"role\": \"user\",
                  \"content\": \"$PROMPT\"
                }
              ],
              \"options\": {
                \"temperature\": 0.1,
                \"mirostat\": 2,
                \"mirostat_tau\": 5.0,
                \"mirostat_eta\": 0.1,
                \"top_p\": 0.95,
                \"repeat_penalty\": 1.15,
                \"num_ctx\": 196608,
                \"num_predict\": 4096
              },
              \"stream\": false
            }")

          # Extract the response content
          UPDATED_PYPROJECT=$(echo "$RESPONSE" | jq -r '.message.content // empty')

          if [ -z "$UPDATED_PYPROJECT" ] || [ "$UPDATED_PYPROJECT" = "null" ]; then
            echo "Error: Empty or invalid response from model"
            exit 1
          fi

          # Validate the response contains valid TOML before writing
          echo "$UPDATED_PYPROJECT" | python3 -c "
          import sys
          try:
              import tomllib
          except ImportError:
              import tomli as tomllib
          try:
              tomllib.loads(sys.stdin.read())
              print('Valid TOML')
          except Exception as e:
              print(f'Invalid TOML: {e}')
              sys.exit(1)
          " || {
            echo "Error: AI response is not valid TOML"
            exit 1
          }
          
          # Write the updated file
          echo "$UPDATED_PYPROJECT" > pyproject.toml
          echo "Updated pyproject.toml successfully"
        env:
          GH_TOKEN: ${{ github.token }}

      - name: Commit and push pyproject.toml update
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          if git diff --quiet pyproject.toml 2>/dev/null; then
            echo "No changes to pyproject.toml"
          else
            git add pyproject.toml
            git commit -m "chore: update coverage threshold to ${{ steps.measure.outputs.actual_coverage }}%

            Enforcing new minimum coverage standard after AI test generation.
            Previous threshold: ${{ needs.analyze.outputs.pyproject_threshold || 'not set' }}%
            New threshold: ${{ steps.measure.outputs.actual_coverage }}%
            
            Generated by generate-tests.yml workflow."
            git push
            echo "âœ… Updated pyproject.toml with new coverage threshold"
          fi

      - name: Comment on PR about coverage update
        if: steps.measure.outputs.actual_coverage != '0'
        env:
          GH_TOKEN: ${{ secrets.CI_GITHUB_TOKEN }}
        run: |
          PR_NUM="${{ needs.consolidate.outputs.pr_number }}"
          REPO="${{ inputs.repository }}"
          PREV_THRESHOLD="${{ needs.analyze.outputs.pyproject_threshold }}"
          EFF_THRESHOLD="${{ needs.analyze.outputs.effective_threshold }}"
          ACTUAL_COV="${{ steps.measure.outputs.actual_coverage }}"
          
          gh pr comment "$PR_NUM" --repo "$REPO" --body "## ðŸ“Š Coverage Standard Updated

          | Metric | Value |
          |--------|-------|
          | Previous threshold | ${PREV_THRESHOLD:-not set}% |
          | Workflow target | ${EFF_THRESHOLD}% |
          | **Achieved coverage** | **${ACTUAL_COV}%** |

          âœ… \`pyproject.toml\` has been updated to enforce the new minimum coverage standard.

          This ensures coverage can only go **UP** in future PRs."

  # ============================================
  # SUMMARY - Final workflow status
  # ============================================
  summary:
    name: Workflow Summary
    needs: [analyze, generate, consolidate, auto-fix, update-pyproject]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Generate final summary
        run: |
          echo "## ðŸŽ¯ Test Generation Workflow Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Repository | \`${{ inputs.repository }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Model | \`${{ inputs.model }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Coverage Threshold (enforced) | ${{ needs.analyze.outputs.effective_threshold }}% |" >> $GITHUB_STEP_SUMMARY
          echo "| Using pyproject.toml? | ${{ inputs.use_pyproject_coverage }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Dry Run | ${{ inputs.dry_run }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Auto-Fix | ${{ inputs.auto_fix_tests }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### Job Status" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Analyze | ${{ needs.analyze.result == 'success' && 'âœ…' || needs.analyze.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.analyze.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Generate | ${{ needs.generate.result == 'success' && 'âœ…' || needs.generate.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.generate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Consolidate | ${{ needs.consolidate.result == 'success' && 'âœ…' || needs.consolidate.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.consolidate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Auto-Fix | ${{ needs.auto-fix.result == 'success' && 'âœ…' || needs.auto-fix.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.auto-fix.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Update pyproject.toml | ${{ needs.update-pyproject.result == 'success' && 'âœ…' || needs.update-pyproject.result == 'skipped' && 'â­ï¸' || 'âŒ' }} ${{ needs.update-pyproject.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.consolidate.outputs.pr_url }}" != "" ]; then
            echo "### ðŸ”— Pull Request" >> $GITHUB_STEP_SUMMARY
            echo "**URL:** ${{ needs.consolidate.outputs.pr_url }}" >> $GITHUB_STEP_SUMMARY
          fi
