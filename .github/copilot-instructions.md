<!-- Generated by Ruler -->


<!-- Source: .ruler/AGENTS.md -->

# AI Agent Guidelines for jbcom Control Center

**This is the control center** for the jbcom Python library ecosystem. It manages multiple packages via a monorepo architecture.

## üö® MANDATORY FIRST: SESSION START

### Session Start Checklist (DO THIS FIRST):
```bash
# 1. Read core agent rules
cat .ruler/AGENTS.md
cat .ruler/fleet-coordination.md

# 2. Check active GitHub Issues for context
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue list --label "agent-session" --state open

# 3. Check your fleet tooling
cd /workspace/packages/cursor-fleet && node dist/cli.js list 2>/dev/null || echo "Fleet not built"
```

### Your Tools:
| Tool | Command | Purpose |
|------|---------|---------|
| Fleet management | `node packages/cursor-fleet/dist/cli.js list` | List Cursor background agents |
| Fleet replay | `node packages/cursor-fleet/dist/cli.js replay <agent-id> -o <dir>` | Recover agent conversation |
| Fleet spawn | `node packages/cursor-fleet/dist/cli.js spawn --repo R --task T` | Spawn agents in repos |

### Session Tracking (USE GITHUB ISSUES):
```bash
# Create session context issue
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue create \
  --label "agent-session" \
  --title "ü§ñ Agent Session: $(date +%Y-%m-%d)" \
  --body "## Context

## Progress

## Blockers"

# Update session progress
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue comment <NUMBER> --body "## Update: ..."

# Close when done
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh issue close <NUMBER>
```

---

## üîë CRITICAL: Authentication (READ FIRST!)

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
```

### Token Reference:
- **GITHUB_JBCOM_TOKEN** - Use for ALL jbcom repo operations (PRs, merges, workflow triggers)
- **CI_GITHUB_TOKEN** - Used by GitHub Actions workflows (in repo secrets)
- **PYPI_TOKEN** - Used by release workflow for PyPI publishing (in repo secrets)

### ‚ö†Ô∏è NEVER FORGET:
The default `GH_TOKEN` does NOT have access to jbcom repos. You MUST prefix with `GH_TOKEN="$GITHUB_JBCOM_TOKEN"` for EVERY `gh` command targeting jbcom repos.

---

## üö® CRITICAL: CI/CD Workflow - THE ACTUAL SYSTEM

### Python Semantic Release (PSR)

**This repository uses Python Semantic Release** for versioning and publishing.

### How It ACTUALLY Works:

```
Push to main branch
  ‚Üì
CI runs all tests
  ‚Üì
semantic-release analyzes commits (conventional commits)
  ‚Üì
IF version bump needed:
  ‚îú‚îÄ‚îÄ Updates version in pyproject.toml
  ‚îú‚îÄ‚îÄ Creates git tag (e.g., extended-data-types-v202511.6.0)
  ‚îú‚îÄ‚îÄ Pushes tag and version commit
  ‚îî‚îÄ‚îÄ Publishes to PyPI
  ‚Üì
IF no version bump needed:
  ‚îî‚îÄ‚îÄ Skips release (no changes detected)
```

### Version Format

**Format**: `YYYYMM.MINOR.PATCH` (e.g., `202511.7.0`)
- Uses CalVer-style YYYYMM prefix
- Minor/patch bumped by conventional commits
- NOT simple auto-increment - uses commit analysis

### Conventional Commits ARE REQUIRED

The release system REQUIRES conventional commit messages:

| Prefix | Effect | Example |
|--------|--------|---------|
| `feat:` | Minor bump | `feat(dic): add decorator API` |
| `fix:` | Patch bump | `fix(vc): resolve import error` |
| `feat!:` or `BREAKING CHANGE:` | Major bump | `feat!: remove deprecated API` |
| `chore:`, `docs:`, `ci:` | No release | `docs: update README` |

### Git Tags ARE USED

**Per-package tags exist:**
```
extended-data-types-v202511.6.0
lifecyclelogging-v202511.6.0
directed-inputs-class-v202511.7.0
vendor-connectors-v202511.10.0
```

### Package Release Order

Packages are released sequentially in dependency order:
1. `extended-data-types` (foundation - no deps)
2. `lifecyclelogging` (depends on EDT)
3. `directed-inputs-class` (depends on EDT)
4. `vendor-connectors` (depends on all above)

---

## üì¶ Monorepo Structure

```
jbcom-control-center/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ extended-data-types/    ‚Üí jbcom/extended-data-types ‚Üí PyPI
‚îÇ   ‚îú‚îÄ‚îÄ lifecyclelogging/       ‚Üí jbcom/lifecyclelogging ‚Üí PyPI
‚îÇ   ‚îú‚îÄ‚îÄ directed-inputs-class/  ‚Üí jbcom/directed-inputs-class ‚Üí PyPI
‚îÇ   ‚îú‚îÄ‚îÄ vendor-connectors/      ‚Üí jbcom/vendor-connectors ‚Üí PyPI
‚îÇ   ‚îú‚îÄ‚îÄ python-terraform-bridge/ ‚Üí (internal, released to PyPI)
‚îÇ   ‚îî‚îÄ‚îÄ cursor-fleet/           ‚Üí (internal tooling, Node.js)
‚îú‚îÄ‚îÄ .ruler/                     ‚Üí Agent rules (SOURCE OF TRUTH)
‚îú‚îÄ‚îÄ .cursor/rules/              ‚Üí Cursor-specific rules
‚îî‚îÄ‚îÄ .github/workflows/          ‚Üí CI/CD pipelines
```

### Edit Code Here

All package code is in `packages/`. Edit directly:
```bash
vim packages/extended-data-types/src/extended_data_types/type_utils.py
vim packages/vendor-connectors/pyproject.toml
```

### Run Tests

```bash
# Using tox (as CI does)
tox -e extended-data-types
tox -e vendor-connectors

# Or directly
cd packages/vendor-connectors && uv run pytest
```

---

## üéØ PR Ownership Rule

**If you are working on a Pull Request:**

- **First agent on PR = PR Owner** - You own ALL feedback, issues, and collaboration
- **Engage with AI agents directly** - Respond to @gemini-code-assist, @copilot, etc.
- **Free the user** - Handle everything that doesn't need human judgment
- **Merge when ready** - Execute merge after all feedback addressed

See `.cursor/rules/05-pr-ownership.mdc` for complete protocol.

---

## ü§ñ For AI Agents: Behavior Guidelines

### Background Agent Behavior

When operating as a **background agent**:

1. **DO NOT** push directly to main branch
2. **DO** create PRs and mark them as ready for review
3. **DO** run all CI checks and fix linting/test failures
4. **DO** respond to PR feedback and iterate
5. **WAIT** for human approval before merging

**EXCEPTION - When User Says:**
> "merge it", "go ahead and merge", "merge to main"

Then you MAY merge PRs after CI passes.

**HOW TO MERGE:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <PR_NUMBER> --squash --delete-branch
```

### Commit Message Rules

Since releases depend on conventional commits:

```bash
# Feature (minor bump)
git commit -m "feat(dic): add new decorator API"

# Fix (patch bump)
git commit -m "fix(vc): resolve authentication bug"

# No release trigger
git commit -m "docs: update README"
git commit -m "chore: update dependencies"

# Use scope for package clarity
# dic = directed-inputs-class
# vc = vendor-connectors
# edt = extended-data-types
# ll = lifecyclelogging
# ptb = python-terraform-bridge
```

---

## üîß Development Workflow

### Local Development

```bash
# Install all packages in dev mode
uv sync --extra dev

# Run tests for specific package
tox -e vendor-connectors

# Run linting
uvx ruff check packages/
uvx ruff format --check packages/
```

### Creating PRs

1. Create a feature branch: `git checkout -b feat/my-feature`
2. Make changes with proper conventional commits
3. Run tests locally: `tox -e <package>`
4. Create PR: `GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create`
5. Wait for CI and address feedback
6. Merge when approved

### Releases

**Releases are automatic on merge to main:**
1. CI runs all checks
2. semantic-release analyzes commits
3. If `feat:` or `fix:` commits found ‚Üí release triggered
4. Package published to PyPI
5. Public repo synced

**Check release status:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/jbcom-control-center --limit 5
```

---

## üìã Quick Reference

### Common Commands

```bash
# List PRs
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr list

# Check CI status
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --limit 5

# Merge PR
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <NUM> --squash --delete-branch

# Check PyPI versions
pip index versions extended-data-types
pip index versions vendor-connectors

# Run tests
tox -e extended-data-types
tox -e vendor-connectors
```

### Package Scopes for Commits

| Package | Scope |
|---------|-------|
| extended-data-types | `edt` |
| lifecyclelogging | `ll` |
| directed-inputs-class | `dic` |
| vendor-connectors | `vc` |
| python-terraform-bridge | `ptb` |
| cursor-fleet | `fleet` |

---

**Last Updated:** 2025-11-30
**Versioning:** Python Semantic Release with CalVer-style format
**Status:** Production - packages released to PyPI



<!-- Source: .ruler/agent-self-sufficiency.md -->

# Agent Self-Sufficiency Rules

**CRITICAL: Read this when you encounter "command not found" or missing tools**

## Core Principle: Tools Should Exist, Use Them

If you encounter a missing tool or command, it usually means ONE of three things:

1. **Tool is in Dockerfile but environment not rebuilt** ‚Üí Document for user
2. **Tool should be in Dockerfile but isn't** ‚Üí ADD IT
3. **Tool is non-standard and shouldn't be assumed** ‚Üí Use alternatives

## Decision Tree: Missing Tool

```
Tool not found
    ‚Üì
Is tool listed in .cursor/TOOLS_REFERENCE.md?
    ‚îú‚îÄ YES ‚Üí Environment needs rebuild
    ‚îÇ         ‚Üí Document in PR/commit: "Requires Docker rebuild"
    ‚îÇ         ‚Üí Continue with workarounds if possible
    ‚îÇ
    ‚îî‚îÄ NO ‚Üí Should this tool be available?
              ‚îú‚îÄ YES ‚Üí ADD to Dockerfile immediately
              ‚îÇ         ‚Üí Common tools (see list below)
              ‚îÇ         ‚Üí Document why it's needed
              ‚îÇ
              ‚îî‚îÄ NO ‚Üí Use standard alternatives
                        ‚Üí python/node/rust/go standard library
                        ‚Üí Tools already in environment
```

## Common Tools That MUST Be Available

### Always Available (Core System)
```bash
# These should ALWAYS work
python --version
node --version  
git --version
bash --version
sh --version
```

### Should Be Available (In Dockerfile)
```bash
# Package managers
pip, uv, pnpm, cargo, go

# Code search
rg (ripgrep), fd, ast-grep

# Data processing  
jq, yq, sqlite3

# Git operations
git, git-lfs, gh, delta, lazygit

# Process management
process-compose, htop, ps, top

# Text processing
bat, exa, vim, nano

# Development
pytest, mypy, ruff, pre-commit

# Agent tools
ruler (for applying agent config changes)
```

### Never Assume Available
```bash
# Don't assume these exist
docker (we're INSIDE docker)
kubectl, helm (cluster tools)
aws, gcloud, az (cloud CLIs - use vendor-connectors)
terraform, pulumi (IaC tools)
```

## When to Add Tools to Dockerfile

### ‚úÖ ADD IMMEDIATELY
- **Standard development tools** everyone needs
- **Security tools** for vulnerability scanning
- **Performance tools** for profiling/debugging
- **Agent management tools** (ruler, etc.)
- **Tools required by project rules** (ripgrep is REQUIRED per .cursorrules)

### ‚ö†Ô∏è ADD WITH JUSTIFICATION
- **Language-specific tools** (add to appropriate section)
- **Build tools** for specific frameworks
- **Testing tools** beyond pytest
- **Database clients** beyond sqlite3

### ‚ùå DON'T ADD
- **Project-specific tools** (install via package.json/pyproject.toml)
- **One-off utilities** (download in CI or use alternatives)
- **Deprecated tools** (find modern alternatives)
- **Redundant tools** (if we have ripgrep, don't add grep alternatives)

## How to Add Tools to Dockerfile

### Pattern 1: System Package (apt)
```dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends \
    tool-name \
    && rm -rf /var/lib/apt/lists/*
```

**Examples**: jq, vim, htop, ripgrep

### Pattern 2: Python Package (pip)
```dockerfile
RUN pip install --no-cache-dir \
    package-name>=X.Y.Z
```

**Examples**: pytest, mypy, ruff

### Pattern 3: Node.js Package (pnpm)
```dockerfile
RUN pnpm install -g \
    package-name
```

**Examples**: @intellectronica/ruler, typescript

### Pattern 4: Rust Tool (cargo)
```dockerfile
RUN cargo install --locked \
    tool-name \
    && rm -rf $CARGO_HOME/registry
```

**Examples**: ripgrep, fd-find, bat, exa

### Pattern 5: Go Tool (go install)
```dockerfile
RUN go install github.com/user/tool@latest
```

**Examples**: yq, lazygit, glow

### Pattern 6: Binary Download
```dockerfile
ENV TOOL_VERSION="vX.Y.Z"
RUN ARCH=$(dpkg --print-architecture) && \
    curl -sSL "https://github.com/user/tool/releases/download/${TOOL_VERSION}/tool-linux-${ARCH}" \
    -o /usr/local/bin/tool && \
    chmod +x /usr/local/bin/tool && \
    tool --version
```

**Examples**: process-compose

## Update Dockerfile Process

When you add a tool:

1. **Choose the right section** in Dockerfile
2. **Add with comment** explaining why
3. **Verify in verification step** at end of Dockerfile
4. **Update TOOLS_REFERENCE.md** with usage examples
5. **Update ENVIRONMENT_ANALYSIS.md** if significant
6. **Document in PR** that Docker rebuild required

### Example: Adding jq (already done correctly)

```dockerfile
# In SYSTEM DEPENDENCIES section
RUN apt-get update && apt-get install -y --no-install-recommends \
    # ... other tools ...
    # JSON/YAML processing
    jq \
    # ... more tools ...
```

```dockerfile
# In FINAL VERIFICATION section  
RUN echo "=== VERIFICATION ===" && \
    # ... other checks ...
    jq --version && \
    # ... more checks ...
```

### Example: Adding ruler (just added)

```dockerfile
# In NODE.JS GLOBAL TOOLS section
RUN pnpm install -g \
    # Ruler - Agent instruction management
    @intellectronica/ruler \
    && ruler --version
```

## Workarounds When Tool Unavailable

### If jq not available
```bash
# Use python instead
python -c "import json, sys; print(json.load(sys.stdin)['key'])" < file.json

# Or for GitHub API
gh api endpoint --jq '.key'  # gh has built-in jq
```

### If yq not available
```bash
# Use python with ruamel.yaml
python -c "import sys; from ruamel.yaml import YAML; yaml=YAML(); print(yaml.load(sys.stdin)['key'])" < file.yaml
```

### If ripgrep not available
```bash
# Fallback to grep (much slower)
grep -r "pattern" .

# But really, add ripgrep - it's REQUIRED by rules
```

### If ruler not available
```bash
# Manual concatenation (not ideal)
cat .ruler/AGENTS.md .ruler/copilot.md .ruler/cursor.md > .cursorrules

# But really, just add ruler to Dockerfile
```

## Self-Healing: Apply Your Own Changes

When you identify a missing tool:

1. **Add it to Dockerfile** immediately (this PR or next commit)
2. **Update documentation** (TOOLS_REFERENCE.md)
3. **Apply agent config changes** with ruler (see below)
4. **Note in commit message**: "Adds <tool> to environment (discovered missing during <task>)"
5. **Verify addition** in verification step
6. **Test locally if possible** or note that rebuild required

### Applying Agent Configuration Changes

**CRITICAL: Cursor reads from `.cursor/rules/*.mdc` files, NOT `.cursorrules`**

When you update agent rules in `.ruler/*.md`:

```bash
# Apply ruler to regenerate all agent configs
ruler apply

# This updates:
# - .cursorrules (for legacy Cursor)
# - .github/copilot-instructions.md (for Copilot)  
# - AGENTS.md (for Aider)
# - .claud (for Claude)
```

**For Cursor background agent, edit these directly:**
- `.cursor/rules/00-loader.mdc` - Project structure and workflow
- `.cursor/rules/05-pr-ownership.mdc` - PR collaboration protocol
- `.cursor/rules/10-background-agent-conport.mdc` - Memory management

**Cursor loads `.mdc` files automatically - no regeneration needed!**

### Example Commit Message
```
build: add ruler to Docker environment

Discovered during PR workflow when attempting to apply agent config
changes. Ruler is essential for maintaining .cursorrules and other
agent-specific configs.

Added as Node.js global via pnpm in NODE.JS GLOBAL TOOLS section.

Requires Docker rebuild: docker build -f .cursor/Dockerfile .
```

## Documentation Updates

When adding tools, update:

### .cursor/TOOLS_REFERENCE.md
```markdown
## New Tool Section

\`\`\`bash
tool-name command            # Description
tool-name --help             # Show help
\`\`\`

### Common Workflows
- Use case 1
- Use case 2
```

### .cursor/ENVIRONMENT_ANALYSIS.md
If significant addition:
```markdown
## Tool Requirements (Update)

### New Category
**New tool** (added YYYY-MM-DD)
- Purpose: Why it's needed
- Installation: How it's installed
- Workflow: What workflow it supports
```

## Anti-Patterns

### ‚ùå Silently Fail
```bash
# Bad: Silently skip if tool missing
which tool && tool command || echo "Skipped"

# Good: Fail explicitly
if ! which tool > /dev/null; then
    echo "ERROR: tool not found. Add to .cursor/Dockerfile"
    exit 1
fi
```

### ‚ùå Install Locally
```bash
# Bad: Install in running container (non-persistent)
apt-get install tool

# Good: Add to Dockerfile (persists across rebuilds)
# Edit .cursor/Dockerfile, document rebuild needed
```

### ‚ùå Assume User Has Tool
```bash
# Bad: Assume tool on user's machine
docker run --rm -v $(which tool):/usr/local/bin/tool ...

# Good: Tool should be in Docker image
# Add to Dockerfile
```

### ‚ùå Use Obscure Tools
```bash
# Bad: Use tool nobody has heard of
obscure-json-parser file.json

# Good: Use standard, well-known tools
jq '.' file.json
```

## Verification Checklist

Before committing Dockerfile changes:

- [ ] Tool added to appropriate section (system deps, python, node, rust, go)
- [ ] Comment explains why tool is needed
- [ ] Version pinned if critical for reproducibility
- [ ] Verification step updated (tool --version check)
- [ ] TOOLS_REFERENCE.md updated with usage
- [ ] ENVIRONMENT_ANALYSIS.md updated if significant
- [ ] Commit message notes Docker rebuild required
- [ ] PR description includes rebuild instructions

## Common Scenarios

### Scenario 1: "ruler: command not found"
**Analysis**: ruler is agent management tool, should be available
**Action**: Add to Dockerfile as Node.js global
**Documentation**: Update TOOLS_REFERENCE.md
**Result**: ‚úÖ Done (just added)

### Scenario 2: "jq: command not found"  
**Analysis**: jq is standard JSON tool, listed in Dockerfile but not in current environment
**Action**: Document rebuild needed, continue with python fallback
**Documentation**: Note in commit/PR
**Result**: ‚è≥ Rebuild required

### Scenario 3: "custom-parser: command not found"
**Analysis**: Project-specific tool, not general-purpose
**Action**: Don't add to Dockerfile, install via package manager in project
**Documentation**: Add to README for that package
**Result**: ‚úÖ Correct approach

### Scenario 4: "docker: command not found"
**Analysis**: We're inside Docker, can't use Docker
**Action**: This is expected, find alternative approach
**Documentation**: Document why Docker-in-Docker not supported
**Result**: ‚úÖ Use different approach

## Summary

1. **Expect tools to exist** - Dockerfile should be comprehensive
2. **Add missing common tools** - Don't work around, fix root cause
3. **Document additions** - Help future agents understand why
4. **Update references** - Keep TOOLS_REFERENCE.md current
5. **Test verification** - Ensure tool checks in Dockerfile work
6. **Self-heal** - You can fix your own environment

**The agent environment should be complete enough that you rarely encounter "command not found" for standard development tasks.**

---

**Version**: 1.0.0  
**Last Updated**: 2025-11-27  
**Related**: `.cursor/Dockerfile`, `.cursor/TOOLS_REFERENCE.md`



<!-- Source: .ruler/copilot.md -->

# GitHub Copilot Configuration

Quick reference for GitHub Copilot when working in jbcom-control-center.

## Quick Rules üö®

### Versioning - Python Semantic Release
‚úÖ Uses conventional commits (`feat:`, `fix:`)
‚úÖ Creates per-package git tags
‚úÖ CalVer-style format: `YYYYMM.MINOR.PATCH`

### Release Process
‚úÖ Merge to main triggers release analysis
‚úÖ `feat:` ‚Üí minor bump, `fix:` ‚Üí patch bump
‚úÖ `chore:`, `docs:` ‚Üí no release

### Code Quality
‚úÖ Type hints required
‚úÖ Tests for new features
‚úÖ Ruff for linting/formatting

## Commit Message Format

**REQUIRED for releases to work:**

```bash
# Feature (minor bump)
feat(edt): add new utility function

# Fix (patch bump)  
fix(vc): resolve authentication bug

# Breaking change (major bump)
feat(dic)!: remove deprecated API

# No release
docs: update README
chore: update dependencies
```

### Package Scopes
- `edt` = extended-data-types
- `ll` = lifecyclelogging
- `dic` = directed-inputs-class
- `vc` = vendor-connectors
- `ptb` = python-terraform-bridge

## Code Patterns

### Prefer Modern Python
```python
# ‚úÖ Good - modern type hints
from collections.abc import Mapping
def func(data: dict[str, Any]) -> list[str]:
    pass

# ‚ùå Avoid - old style
from typing import Dict, List
def func(data: Dict[str, Any]) -> List[str]:
    pass
```

### Use Pathlib
```python
# ‚úÖ Good
from pathlib import Path
config_file = Path("config.yaml")

# ‚ùå Avoid
import os
config_file = os.path.join("config.yaml")
```

### Type Hints Required
```python
# ‚úÖ Good
def process_data(items: list[dict[str, Any]]) -> dict[str, int]:
    """Process items and return counts."""
    return {"count": len(items)}

# ‚ùå Avoid
def process_data(items):
    return {"count": len(items)}
```

## Testing Patterns

### Write Clear Tests
```python
# ‚úÖ Good - descriptive name
def test_process_data_returns_correct_count():
    items = [{"id": 1}, {"id": 2}]
    result = process_data(items)
    assert result["count"] == 2

# ‚ùå Avoid - vague name
def test_stuff():
    result = process_data([{"id": 1}])
    assert result
```

### Use Fixtures
```python
@pytest.fixture
def sample_data():
    return [{"id": i} for i in range(10)]

def test_with_fixture(sample_data):
    result = process_data(sample_data)
    assert result["count"] == 10
```

## Common Tasks

### Adding a New Function
1. Write function with type hints
2. Add docstring (Google style)
3. Write tests
4. Update module `__all__` if public API
5. Run `ruff check` and `pytest`
6. Commit with `feat(scope):` message

### Fixing a Bug
1. Write test that reproduces bug
2. Fix the bug
3. Verify test passes
4. Commit with `fix(scope):` message

## Documentation - Google Style
```python
def process_items(items: list[dict], validate: bool = True) -> dict[str, Any]:
    """Process a list of items and return summary.

    Args:
        items: List of dictionaries containing item data
        validate: Whether to validate items before processing

    Returns:
        Dictionary with processing summary and statistics

    Raises:
        ValueError: If items list is empty or validation fails
    """
```

## Security

### Never Log Secrets
```python
# ‚úÖ Good
safe_config = {k: v for k, v in config.items() if k != "api_key"}
logger.info(f"Config: {safe_config}")

# ‚ùå Avoid
logger.info(f"Config: {config}")
```

---

**Copilot Instructions Version:** 2.0
**Last Updated:** 2025-11-30



<!-- Source: .ruler/cursor.md -->

# Cursor-Specific Agent Configuration

Configuration for Cursor AI agents in jbcom-control-center.

## üîë Authentication

**ALWAYS use `GITHUB_JBCOM_TOKEN` for ALL jbcom repo operations:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --title "..." --body "..."
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge 123 --squash --delete-branch
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh run list --repo jbcom/extended-data-types
```

The default `GH_TOKEN` does NOT have jbcom access.

## üö® Long-Running PR Workflow (Hold-Open Pattern)

When managing **multiple merges to main** and **multiple CI runs**, use the hold-open pattern:

### The Problem
When a background agent creates a PR and merges it, the session closes because the branch is deleted.

### The Solution: Holding PR + Interim PRs

**1. Create Holding PR (keeps session alive):**
```bash
git checkout -b agent/holding-pr-for-<task>-$(date +%Y%m%d-%H%M%S)
echo "# Agent Session" >> .cursor/agents/session-notes.md
git commit -m "Agent holding PR for <task>"
git push -u origin HEAD
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create --draft \
  --title "[HOLDING] Agent session for <task>" \
  --body "Keeps session alive. DO NOT MERGE until work complete."
```

**2. Create Interim PRs (for actual fixes):**
```bash
git checkout main && git pull
git checkout -b fix/<specific-issue>
# Make the fix
git commit -m "fix(scope): specific issue"
git push -u origin HEAD
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create
# After CI passes
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <NUM> --squash --delete-branch
```

**3. When Done:**
```bash
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr close <HOLDING_PR_NUM>
```

### Rules for Hold-Open PRs
- Use `--draft` to avoid triggering AI reviewers
- Title with `[HOLDING]` prefix
- NEVER merge the holding PR until all work complete
- Create NEW interim branches from updated main for each fix

## üõ†Ô∏è cursor-fleet Tooling

The fleet tooling is in `packages/cursor-fleet/`. Build and use:

```bash
# Build
cd /workspace/packages/cursor-fleet && npm run build

# List agents
node dist/cli.js list

# Get agent status
node dist/cli.js status <agent-id>

# Replay/recover conversation
node dist/cli.js replay <agent-id> -o /workspace/.cursor/recovery/<agent-id> -v

# Spawn new agent
node dist/cli.js spawn --repo jbcom/vendor-connectors --task "Fix CI"

# Send followup
node dist/cli.js followup <agent-id> "Please check PR feedback"
```

### Replay Features
- Fetches full conversation via Cursor API
- Splits into batches (10 messages per file)
- Creates INDEX.md for navigation
- Extracts completed/outstanding tasks
- Archives to specified directory

## Background Agent Modes

### Code Review Mode
When reviewing PRs:
- Focus on logic and correctness
- Check type safety
- Verify test coverage
- Look for security issues

### Maintenance Mode
For routine tasks:
- Update dependencies
- Fix linting issues
- Improve documentation
- Keep it simple

### Ecosystem Coordination Mode
When working across packages:
- Work in dependency order (EDT ‚Üí LL ‚Üí DIC ‚Üí VC)
- Test each package independently
- Create PRs for each change
- Ensure CI passes before merge

## Error Handling

### CI Failures
1. Read full error output
2. Identify root cause
3. Fix the issue
4. Push fix
5. Verify CI passes

### Type Check Errors
1. Read the specific error
2. Fix with proper type hints
3. Don't use `type: ignore` unless necessary

### Test Failures
1. Read test output carefully
2. Identify which test failed
3. Fix the code or the test
4. Run full test suite

## Code Style

### Python Style
- Modern type hints: `list[]`, `dict[]`, not `List[]`, `Dict[]`
- Prefer pathlib over os.path
- Use context managers for resources
- Keep functions focused and small
- Docstrings for public APIs

### Commit Messages
Use conventional commits for releases to work:
```bash
feat(vc): add new connector       # Minor bump
fix(dic): resolve bug             # Patch bump
docs: update README               # No release
```

## Communication Style

### With User
- Be concise
- Highlight important info
- Use formatting for clarity
- Show progress on long tasks

### In Code Comments
- Explain why, not what
- Link to issues/PRs for context
- Keep them brief

---

**Cursor Version:** Compatible with latest Cursor AI
**Last Updated:** 2025-11-30



<!-- Source: .ruler/ecosystem.md -->

# Ecosystem Repositories

This control center manages the jbcom Python library ecosystem via **MONOREPO ARCHITECTURE**.

---

## üèóÔ∏è ARCHITECTURE: All Code Lives Here

**ALL Python ecosystem code is in `packages/` in this repository.**

```
jbcom-control-center/packages/
‚îú‚îÄ‚îÄ extended-data-types/    ‚Üí syncs to ‚Üí jbcom/extended-data-types ‚Üí PyPI
‚îú‚îÄ‚îÄ lifecyclelogging/       ‚Üí syncs to ‚Üí jbcom/lifecyclelogging ‚Üí PyPI
‚îú‚îÄ‚îÄ directed-inputs-class/  ‚Üí syncs to ‚Üí jbcom/directed-inputs-class ‚Üí PyPI
‚îî‚îÄ‚îÄ vendor-connectors/      ‚Üí syncs to ‚Üí jbcom/vendor-connectors ‚Üí PyPI
```

### Workflow
1. **Edit** code in `packages/`
2. **PR** to control-center main
3. **Sync workflow** creates PRs in public repos
4. **Merge** public PRs ‚Üí CI ‚Üí PyPI release

### Why Monorepo
- ‚úÖ No cloning external repos
- ‚úÖ No GitHub API gymnastics
- ‚úÖ Single source of truth
- ‚úÖ Cross-package changes in ONE PR
- ‚úÖ Dependencies always aligned

---

## üì¶ Package Details

### 1. extended-data-types (FOUNDATION)
**Location:** `packages/extended-data-types/`
**PyPI:** `extended-data-types`
**Public Repo:** `jbcom/extended-data-types`

The foundation library - ALL other packages depend on this.

**Provides:**
- Re-exported libraries: `gitpython`, `inflection`, `lark`, `orjson`, `python-hcl2`, `ruamel.yaml`, `sortedcontainers`, `wrapt`
- Utilities: `strtobool`, `strtopath`, `make_raw_data_export_safe`, `get_unique_signature`
- Serialization: `decode_yaml`, `encode_yaml`, `decode_json`, `encode_json`
- Collections: `flatten_map`, `filter_map`, and more

**Rule:** Before adding ANY dependency to other packages, check if extended-data-types provides it.

### 2. lifecyclelogging
**Location:** `packages/lifecyclelogging/`
**PyPI:** `lifecyclelogging`
**Public Repo:** `jbcom/lifecyclelogging`

Structured lifecycle logging with automatic sanitization.

**Depends on:** extended-data-types

### 3. directed-inputs-class
**Location:** `packages/directed-inputs-class/`
**PyPI:** `directed-inputs-class`
**Public Repo:** `jbcom/directed-inputs-class`

Declarative input validation and processing.

**Depends on:** extended-data-types

### 4. vendor-connectors
**Location:** `packages/vendor-connectors/`
**PyPI:** `vendor-connectors`
**Public Repo:** `jbcom/vendor-connectors`

Unified vendor connectors (AWS, GCP, GitHub, Slack, Vault, Zoom).

**Depends on:** extended-data-types, lifecyclelogging, directed-inputs-class

---

## üîó Dependency Chain

```
extended-data-types (FOUNDATION)
‚îú‚îÄ‚îÄ lifecyclelogging
‚îú‚îÄ‚îÄ directed-inputs-class
‚îî‚îÄ‚îÄ vendor-connectors (depends on BOTH)
```

**Release Order:** Always release in this order:
1. extended-data-types
2. lifecyclelogging
3. directed-inputs-class
4. vendor-connectors

---

## üîß Working With Packages

### Edit Code
```bash
# Just edit files directly!
vim packages/extended-data-types/src/extended_data_types/type_utils.py
vim packages/vendor-connectors/pyproject.toml
```

### Run Tests
```bash
cd packages/extended-data-types && pip install -e ".[tests]" && pytest
cd packages/lifecyclelogging && pip install -e ".[tests]" && pytest
```

### Align Dependencies
```bash
# Update version across all packages
sed -i 's/extended-data-types>=.*/extended-data-types>=2025.11.200/' \
  packages/*/pyproject.toml
```

### Create PR
```bash
git checkout -b fix/whatever
git add -A && git commit -m "Fix: description"
git push -u origin fix/whatever
gh pr create --title "Fix: whatever"
```

---

## üîÑ Sync Configuration

### Files
- `packages/ECOSYSTEM.toml` - Source of truth
- `.github/sync.yml` - What syncs where
- `.github/workflows/sync-packages.yml` - Sync workflow

### Triggers
- Push to main with `packages/**` changes
- Manual workflow dispatch
- Release published

### Secret
`CI_GITHUB_TOKEN` from Doppler - has write access to all jbcom repos

---

## ‚ö†Ô∏è Rules

### DO
- ‚úÖ Edit code in `packages/` directly
- ‚úÖ Use regular git for this repo
- ‚úÖ Check `packages/ECOSYSTEM.toml` for relationships
- ‚úÖ Use extended-data-types utilities
- ‚úÖ Release in dependency order

### DON'T
- ‚ùå Clone external repos - code is HERE
- ‚ùå Add duplicate utilities
- ‚ùå Skip the sync workflow
- ‚ùå Push directly to main (use PRs)

---

## üéØ Eliminate Duplication

### Check Before Adding Dependencies
Always check `packages/extended-data-types/pyproject.toml` first.

### Red Flags
- `utils.py` > 100 lines ‚Üí duplicating extended-data-types
- Direct `import inflection` ‚Üí should use extended-data-types
- Custom JSON/YAML functions ‚Üí use `encode_json`, `decode_yaml`

### Correct Pattern
```python
# ‚úÖ Use foundation library
from extended_data_types import strtobool, make_raw_data_export_safe

# ‚ùå Don't reimplement
def my_str_to_bool(val):
    return val.lower() in ("true", "yes", "1")
```

---

## üìä Health Checks

### Check Public Repo CI
```bash
for repo in extended-data-types lifecyclelogging directed-inputs-class vendor-connectors; do
  gh run list --repo jbcom/$repo --limit 3
done
```

### Check PyPI Versions
```bash
pip index versions extended-data-types
pip index versions lifecyclelogging
pip index versions vendor-connectors
```

### Trigger Manual Sync
```bash
gh workflow run "Sync Packages to Public Repos" --repo jbcom/jbcom-control-center
```

---

**Source of Truth:** `packages/ECOSYSTEM.toml`
**All code is in:** `packages/`
**Sync handles:** Pushing to public repos and PyPI



<!-- Source: .ruler/environment-setup.md -->

# Development Environment Setup Guide for Agents

## Overview

This workspace can run in **TWO DIFFERENT ENVIRONMENTS**:

1. **Cursor Environment** - Dockerized environment for Cursor IDE background agents
2. **GitHub Actions Environment** - Native Ubuntu runners for CI/CD workflows

**CRITICAL:** You must understand which environment you're in and adapt accordingly.

---

## Environment Detection

Check your environment:

```bash
# Are we in Docker (Cursor)?
if [ -f /.dockerenv ]; then
    echo "üê≥ Running in Cursor Docker environment"
else
    echo "üîß Running in GitHub Actions or native environment"
fi
```

---

## 1. Cursor Docker Environment

### What's Pre-installed

The `.cursor/Dockerfile` provides:
- **Languages:** Python 3.13, Node.js 24
- **Package Managers:** uv (Python), pnpm (Node.js)
- **System Tools:** git, gh CLI, just, sqlite3, ripgrep, fd, jq, vim, nano
- **Build Tools:** gcc, make, pkg-config (for native modules)

### What's NOT Pre-installed

**You must install these yourself when needed:**
- Python packages (use `uv sync`)
- Node.js packages (use `pnpm install`)
- Any application-specific tools (ruler, mcp-proxy, playwright, etc.)

### How to Set Up Workspace

**Option 1: Automatic with direnv**
```bash
# Install direnv if not available (unlikely in Docker)
direnv allow
# This automatically runs .envrc which:
# - Creates Python venv with UV
# - Installs Python dev dependencies
# - Installs Node.js dependencies
# - Sets up PATH
```

**Option 2: Manual Setup**
```bash
# Python environment
uv sync --extra dev --all-extras
source .venv/bin/activate

# Node.js environment  
pnpm install
export PATH="$PWD/node_modules/.bin:$PATH"
```

### Installing Additional Tools

**Node.js tools from package.json:**
```bash
# Tools are defined in package.json devDependencies
# After pnpm install, they're available in node_modules/.bin/

# Example: Run ruler
pnpm exec ruler --version
# or
./node_modules/.bin/ruler --version
```

**One-off Node.js tools:**
```bash
# Use pnpm dlx (like npx) for tools not in package.json
pnpm dlx playwright@latest install chromium
```

**Python tools:**
```bash
# Install additional Python tools if needed
uv add --dev some-tool
# or use uvx for one-off commands
uvx ruff check .
```

---

## 2. GitHub Actions Environment

### What's Pre-installed

GitHub-hosted runners come with:
- Python (multiple versions available via actions/setup-python)
- Node.js (multiple versions available via actions/setup-node)
- Common tools: git, curl, wget, jq, etc.

See full list: https://github.com/actions/runner-images/blob/main/images/ubuntu/Ubuntu2204-Readme.md

### What You Must Install

**EVERYTHING** workspace-specific:
- uv (via `astral-sh/setup-uv@v7`)
- pnpm (via corepack)
- Python dependencies (via `uv sync`)
- Node.js dependencies (via `pnpm install`)
- Any application tools

### Workflow Pattern

Our CI workflows follow this pattern:

```yaml
- uses: actions/checkout@v4

# Set up UV (for Python)
- uses: astral-sh/setup-uv@v7

# Set up pnpm (for Node.js)
- run: corepack enable && corepack prepare pnpm@9.15.0 --activate

# Install dependencies
- run: uv sync --extra dev
- run: pnpm install

# Now workspace tools are available
- run: uv run pytest
- run: pnpm exec ruler apply
```

---

## Understanding Package Management

### Python with UV (Similar to Rust's Cargo)

**pyproject.toml** = Manifest file
- Defines project metadata
- Lists dependencies
- Defines workspace members (`packages/*`)
- Dev dependencies in `[project.optional-dependencies.dev]`

**uv.lock** = Lock file (COMMITTED)
- Pins exact versions for reproducibility
- Like `Cargo.lock` or `pnpm-lock.yaml`

**Commands:**
```bash
# Install everything (respects uv.lock)
uv sync --extra dev

# Add a dependency
uv add requests

# Add a dev dependency  
uv add --dev pytest

# Run a command in the venv
uv run pytest

# Run a one-off tool without installing
uvx ruff check .
```

### Node.js with pnpm (Workspace-aware)

**package.json** = Manifest file
- Defines project metadata
- Lists devDependencies (tools like ruler, playwright)
- Specifies `packageManager: "pnpm@9.15.0"`

**pnpm-workspace.yaml** = Workspace configuration
- Can define multiple packages (currently none)
- Similar to UV's workspace concept

**pnpm-lock.yaml** = Lock file (COMMITTED)
- Pins exact versions
- Like `uv.lock`

**.npmrc** = pnpm configuration
- Workspace settings
- Registry configuration

**Commands:**
```bash
# Install everything (respects pnpm-lock.yaml)
pnpm install

# Add a dev dependency
pnpm add -D some-tool

# Run a tool from node_modules/.bin
pnpm exec ruler apply

# Run a one-off tool without installing
pnpm dlx playwright@latest install chromium
```

---

## Common Tasks by Environment

### Installing Ruler (Agent Instruction Tool)

**Cursor Docker:**
```bash
# Ruler is in package.json devDependencies
pnpm install
pnpm exec ruler --version
```

**GitHub Actions:**
```yaml
- name: Install ruler
  run: |
    corepack enable
    corepack prepare pnpm@9.15.0 --activate
    pnpm install
    pnpm exec ruler --version
```

### Running Tests

**Cursor Docker:**
```bash
# Python tests
uv sync --extra dev
uv run pytest

# If you have JS tests
pnpm install
pnpm test
```

**GitHub Actions:**
```yaml
- uses: astral-sh/setup-uv@v7
- run: uv sync --extra dev  
- run: uv run pytest
```

### Installing Playwright

**Cursor Docker:**
```bash
# One-time browser installation
pnpm dlx playwright@1.49.0 install chromium

# Or if you need it frequently, add to package.json:
pnpm add -D playwright
pnpm exec playwright install chromium
```

**GitHub Actions:**
```yaml
- name: Install Playwright
  run: |
    pnpm add -D playwright
    pnpm exec playwright install chromium --with-deps
```

---

## When You Need a Tool

### Decision Tree:

1. **Is it a Python tool for development?**
   - Add to `pyproject.toml` under `[project.optional-dependencies.dev]`
   - Run `uv sync --extra dev`

2. **Is it a Node.js tool for development?**
   - Add to `package.json` under `devDependencies`
   - Run `pnpm install`

3. **Is it a system tool everyone needs?**
   - Add to `.cursor/Dockerfile` (system packages via apt)
   - Document requirement for GitHub Actions in workflow

4. **Is it a one-off tool?**
   - Use `uvx` (Python) or `pnpm dlx` (Node.js)
   - Don't install globally

### Examples:

**Python formatting with Ruff:**
```bash
# Already in package.json dev deps
uv run ruff check .
uv run ruff format .
```

**TypeScript type checking:**
```bash
# Add if not present
pnpm add -D typescript
pnpm exec tsc --noEmit
```

**Running a script:**
```bash
# Python script
uv run python scripts/my_script.py

# Node.js script  
pnpm exec ts-node scripts/my_script.ts
```

---

## Environment Variables

Both environments should respect:

```bash
# Python
export PYTHONUNBUFFERED=1
export PYTHONDONTWRITEBYTECODE=1

# Telemetry
export DO_NOT_TRACK=1
export DISABLE_TELEMETRY=1

# Package managers
export UV_LINK_MODE=copy  # For UV
export PNPM_HOME=/root/.local/share/pnpm  # For pnpm
```

These are set in:
- `.cursor/Dockerfile` (for Cursor environment)
- `.envrc` (for direnv users)
- Workflows (for GitHub Actions)

---

## Troubleshooting

### "Command not found: ruler"

**Cursor:** Run `pnpm install` first
**GitHub Actions:** Add pnpm setup to workflow

### "Module not found: extended_data_types"

**Both:** Run `uv sync` to install workspace packages

### "ENOENT: no such file or directory, open 'pnpm-lock.yaml'"

**Both:** Run `pnpm install` to generate lock file

### "uv: command not found"

**Cursor:** Should never happen (pre-installed in Dockerfile)
**GitHub Actions:** Add `uses: astral-sh/setup-uv@v7` to workflow

---

## Best Practices

1. **Always use lock files**
   - Commit `uv.lock` and `pnpm-lock.yaml`
   - Never use `--no-lock` or `--frozen-lockfile` unless necessary

2. **Use workspace dependencies**
   - Python packages reference each other via `{ workspace = true }`
   - This ensures you're testing against local code, not PyPI

3. **Prefer workspace tools over global**
   - Don't install tools globally in Dockerfile
   - Define them in package.json/pyproject.toml
   - This makes dependencies explicit and version-controlled

4. **Document new requirements**
   - Update this file when adding new tools
   - Update Dockerfile if system packages needed
   - Update CI workflows if new setup steps required

---

## Quick Reference

### Cursor Docker Environment
```bash
# Setup
uv sync --extra dev
pnpm install

# Common commands
uv run pytest
pnpm exec ruler apply
uv run ruff check .
pnpm exec playwright test
```

### GitHub Actions
```yaml
# Setup
- uses: astral-sh/setup-uv@v7
- run: corepack enable && corepack prepare pnpm@9.15.0 --activate
- run: uv sync --extra dev && pnpm install

# Use
- run: uv run pytest
- run: pnpm exec ruler apply
```

### Adding Dependencies
```bash
# Python
uv add requests              # Production
uv add --dev pytest-cov      # Development

# Node.js
pnpm add axios               # Production
pnpm add -D typescript       # Development
```



<!-- Source: .ruler/fleet-coordination.md -->

# Fleet Coordination

## cursor-fleet Package

The `@jbcom/cursor-fleet` package in `packages/cursor-fleet/` provides Cursor Background Agent management.

### Building

```bash
cd /workspace/packages/cursor-fleet
npm install
npm run build
```

### CLI Commands

```bash
# List all agents
node dist/cli.js list

# Get agent status
node dist/cli.js status <agent-id>

# Get agent conversation
node dist/cli.js conversation <agent-id>

# Replay/recover agent (with splitting)
node dist/cli.js replay <agent-id> -o <output-dir> -v

# Split existing conversation.json
node dist/cli.js split <conversation.json> -o <output-dir>

# Spawn new agent
node dist/cli.js spawn --repo owner/repo --task "Task description"

# Send followup message
node dist/cli.js followup <agent-id> "Message"

# Archive agent data
node dist/cli.js archive <agent-id> -o <output-dir>
```

### Replay Features

The `replay` command provides comprehensive conversation recovery:

```bash
node dist/cli.js replay bc-7f35d6f6-a052-4f88-9dba-252d359b8395 \
  -o /workspace/.cursor/recovery/bc-7f35d6f6-a052-4f88-9dba-252d359b8395 \
  -v
```

**Output structure:**
```
<output-dir>/
‚îú‚îÄ‚îÄ conversation.json    # Full conversation
‚îú‚îÄ‚îÄ agent.json           # Agent metadata
‚îú‚îÄ‚îÄ analysis.json        # Extracted tasks/PRs
‚îú‚îÄ‚îÄ metadata.json        # Split metadata
‚îú‚îÄ‚îÄ INDEX.md             # Message index with links
‚îú‚îÄ‚îÄ REPLAY_SUMMARY.md    # Human-readable summary
‚îú‚îÄ‚îÄ messages/            # Individual message files
‚îÇ   ‚îú‚îÄ‚îÄ 0001-USER.md
‚îÇ   ‚îú‚îÄ‚îÄ 0001-USER.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ batches/             # Batch files (10 messages each)
    ‚îú‚îÄ‚îÄ batch-001.md
    ‚îú‚îÄ‚îÄ batch-001.json
    ‚îî‚îÄ‚îÄ ...
```

**Analysis extracts:**
- Completed tasks (‚úÖ patterns)
- Outstanding tasks (‚è≥ patterns)
- PRs created/merged
- Blockers identified
- Key decisions

## Hold-Open PR Pattern

For multi-merge sessions, create a **draft PR** as a coordination channel:

```bash
# 1. Create holding branch
git checkout -b agent/holding-session-$(date +%Y%m%d-%H%M%S)
echo "# Session Notes" >> .cursor/agents/session.md
git add -A && git commit -m "chore: agent holding PR"
git push -u origin HEAD

# 2. Create as DRAFT to avoid AI reviewers
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create \
  --draft \
  --title "[HOLDING] Agent session (DO NOT MERGE)" \
  --body "Communication channel. DO NOT MERGE until complete."

# 3. Work via interim PRs
git checkout main && git pull
git checkout -b fix/specific-issue
# ... make fix ...
git push -u origin HEAD
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr create
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr merge <NUM> --squash --delete-branch

# 4. When done, close holding PR
GH_TOKEN="$GITHUB_JBCOM_TOKEN" gh pr close <HOLDING_PR_NUM>
```

**Why draft?** Prevents Amazon Q, Gemini, CodeRabbit from reviewing the holding PR.

## Agent Reporting Protocol

Sub-agents can report status via PR comments:

| Format | Meaning |
|--------|---------|
| `@cursor ‚úÖ DONE: [summary]` | Task completed |
| `@cursor ‚ö†Ô∏è BLOCKED: [issue]` | Needs intervention |
| `@cursor üìä STATUS: [progress]` | Progress update |

## API Classes

### CursorAPI (Direct)

```typescript
import { CursorAPI } from "@jbcom/cursor-fleet";

const api = new CursorAPI(process.env.CURSOR_API_KEY);

// List agents
const agents = await api.listAgents();

// Get status
const agent = await api.getAgentStatus("bc-xxx");

// Get conversation
const conversation = await api.getAgentConversation("bc-xxx");

// Launch agent
const newAgent = await api.launchAgent({
  prompt: { text: "Fix the CI" },
  source: { repository: "jbcom/vendor-connectors" }
});

// Send followup
await api.addFollowup("bc-xxx", { text: "Check PR feedback" });
```

### Fleet (High-level)

```typescript
import { Fleet } from "@jbcom/cursor-fleet";

const fleet = new Fleet({ apiKey: process.env.CURSOR_API_KEY });

// Replay with splitting
const result = await fleet.replay("bc-xxx", {
  outputDir: "/path/to/output",
  verbose: true
});

// Split existing conversation
await fleet.splitExisting("/path/to/conversation.json", "/path/to/output");

// Load previous replay
const data = await fleet.loadReplay("/path/to/archive");
```

## Environment Variables

```bash
export CURSOR_API_KEY="..."  # Required for API calls
export GITHUB_JBCOM_TOKEN="..." # For GitHub operations
```

## process-compose Integration

Add to `process-compose.yml` for long-running coordination:

```yaml
fleet-watcher:
  command: |
    node /workspace/packages/cursor-fleet/dist/cli.js list --json | \
    jq -r '.[] | select(.status == "RUNNING") | .id'
  depends_on: []
```

---

**Package:** @jbcom/cursor-fleet
**Location:** packages/cursor-fleet/
**Last Updated:** 2025-11-30



<!-- Source: .ruler/README.md -->

# Ruler Directory - AI Agent Instructions

This directory contains the **single source of truth** for all AI agent instructions.

## How Ruler Works

Ruler centralizes AI agent instructions in `.ruler/*.md` files and generates agent-specific configuration files.

### Processing Order

1. `AGENTS.md` - Core guidelines (always first)
2. Remaining `.md` files in sorted order

### Files

| File | Purpose |
|------|---------|
| `AGENTS.md` | Core guidelines, versioning, monorepo structure |
| `copilot.md` | GitHub Copilot patterns and quick reference |
| `cursor.md` | Cursor agent modes, hold-open PR pattern |
| `ecosystem.md` | Package relationships and coordination |
| `fleet-coordination.md` | cursor-fleet tooling documentation |
| `environment-setup.md` | Dev environment setup |
| `agent-self-sufficiency.md` | Tool availability and self-healing |

### Output Files (Generated - DO NOT edit)

- `.cursorrules` - Cursor AI
- `.claud` - Claude Code
- `.github/copilot-instructions.md` - GitHub Copilot
- `AGENTS.md` (root) - Aider and general AI agents

## Making Changes

### Update Agent Instructions

1. **Edit files in `.ruler/`:**
   ```bash
   vim .ruler/AGENTS.md
   vim .ruler/cursor.md
   ```

2. **Apply ruler:**
   ```bash
   pnpm exec ruler apply
   ```

3. **Review changes:**
   ```bash
   git diff .cursorrules AGENTS.md
   ```

4. **Commit:**
   ```bash
   git add .ruler/ .cursorrules .github/copilot-instructions.md AGENTS.md
   git commit -m "docs: update agent instructions"
   ```

### Configuration

Edit `.ruler/ruler.toml`:

```toml
default_agents = ["copilot", "cursor", "claude", "aider"]

[agents.copilot]
enabled = true
output_path = ".github/copilot-instructions.md"

[agents.cursor]
enabled = true
# Uses default .cursorrules

[agents.claude]
enabled = true
# Uses default .claud

[agents.aider]
enabled = true
output_path_instructions = "AGENTS.md"
```

## Key Content

### Versioning (AGENTS.md)
- Uses Python Semantic Release
- Per-package git tags
- Conventional commits required
- CalVer-style format: `YYYYMM.MINOR.PATCH`

### Fleet Tooling (fleet-coordination.md)
- cursor-fleet CLI for agent management
- Replay/recovery of conversations
- Hold-open PR pattern for long sessions

### Code Quality (copilot.md)
- Type hints required
- Google-style docstrings
- Ruff for linting

## Best Practices

1. **Be explicit** - Don't assume agents understand context
2. **Use examples** - Show both good and bad patterns
3. **Explain why** - Not just what to do
4. **Keep updated** - Revise based on agent behavior

---

**Ruler Version:** Compatible with @intellectronica/ruler latest
**Last Updated:** 2025-11-30
